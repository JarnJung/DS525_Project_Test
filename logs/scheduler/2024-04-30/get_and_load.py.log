[2024-04-30T18:32:15.322+0000] {processor.py:161} INFO - Started process (PID=2045274) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:32:15.325+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:32:15.327+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:32:15.326+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:32:15.380+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:32:15.364+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:32:15.382+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:32:15.439+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.128 seconds
[2024-04-30T18:32:45.594+0000] {processor.py:161} INFO - Started process (PID=2045310) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:32:45.597+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:32:45.600+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:32:45.599+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:32:45.638+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:32:45.619+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:32:45.639+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:32:45.696+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T18:33:15.813+0000] {processor.py:161} INFO - Started process (PID=2045346) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:33:15.816+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:33:15.819+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:33:15.818+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:33:15.857+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:33:15.839+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:33:15.858+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:33:15.915+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T18:33:46.212+0000] {processor.py:161} INFO - Started process (PID=2045382) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:33:46.215+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:33:46.218+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:33:46.217+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:33:46.256+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:33:46.238+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:33:46.257+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:33:46.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T18:34:16.399+0000] {processor.py:161} INFO - Started process (PID=2045418) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:34:16.415+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:34:16.418+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:34:16.417+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:34:16.479+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:34:16.462+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:34:16.480+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:34:16.568+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.180 seconds
[2024-04-30T18:34:46.794+0000] {processor.py:161} INFO - Started process (PID=2045454) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:34:46.797+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:34:46.800+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:34:46.799+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:34:46.846+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:34:46.824+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:34:46.848+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:34:46.933+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.150 seconds
[2024-04-30T18:34:50.117+0000] {processor.py:161} INFO - Started process (PID=2045459) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:34:50.121+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:34:50.125+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:34:50.124+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:34:50.183+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:34:50.163+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:34:50.185+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:34:50.241+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.140 seconds
[2024-04-30T18:35:20.398+0000] {processor.py:161} INFO - Started process (PID=2045495) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:35:20.400+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:35:20.403+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:35:20.402+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:35:20.442+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:35:20.423+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:35:20.443+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:35:20.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.112 seconds
[2024-04-30T18:35:50.900+0000] {processor.py:161} INFO - Started process (PID=2045531) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:35:50.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:35:50.907+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:35:50.906+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:35:50.947+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:35:50.927+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:35:50.948+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:35:51.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.112 seconds
[2024-04-30T18:36:21.467+0000] {processor.py:161} INFO - Started process (PID=2045567) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:36:21.470+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:36:21.473+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:36:21.472+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:36:21.512+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:36:21.493+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:36:21.513+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:36:21.568+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.111 seconds
[2024-04-30T18:36:51.987+0000] {processor.py:161} INFO - Started process (PID=2045603) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:36:51.991+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:36:51.994+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:36:51.993+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:36:52.032+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:36:52.016+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:36:52.033+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:36:52.087+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.110 seconds
[2024-04-30T18:39:28.812+0000] {processor.py:161} INFO - Started process (PID=172) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:39:28.872+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:39:28.883+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:39:28.881+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:39:28.933+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:39:28.927+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:39:28.934+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:39:29.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.215 seconds
[2024-04-30T18:39:59.618+0000] {processor.py:161} INFO - Started process (PID=208) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:39:59.621+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:39:59.628+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:39:59.626+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:39:59.656+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:39:59.650+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:39:59.658+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:39:59.737+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.135 seconds
[2024-04-30T18:40:30.090+0000] {processor.py:161} INFO - Started process (PID=244) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:40:30.095+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:40:30.104+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:40:30.102+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:40:30.134+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:40:30.128+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:40:30.136+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:40:30.204+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.128 seconds
[2024-04-30T18:41:00.397+0000] {processor.py:161} INFO - Started process (PID=281) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:41:00.402+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:41:00.409+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:41:00.408+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:41:00.435+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:41:00.429+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:41:00.436+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:41:00.506+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.125 seconds
[2024-04-30T18:41:30.766+0000] {processor.py:161} INFO - Started process (PID=317) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:41:30.775+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:41:30.781+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:41:30.780+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:41:30.807+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:41:30.801+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:41:30.808+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:41:30.874+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.123 seconds
[2024-04-30T18:42:01.013+0000] {processor.py:161} INFO - Started process (PID=353) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:42:01.017+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:42:01.025+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:42:01.023+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:42:01.053+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:42:01.047+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 176, in <module>
    load_data_to_bq = PythonOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 185, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 811, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/task_group.py", line 231, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'extract_data' has already been added to the DAG
[2024-04-30T18:42:01.055+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:42:01.122+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.124 seconds
[2024-04-30T18:42:30.549+0000] {processor.py:161} INFO - Started process (PID=389) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:42:30.553+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:42:30.556+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:42:30.555+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:42:30.604+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:42:30.957+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:42:30.955+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:load_data_to_bq
[2024-04-30T18:42:30.987+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:42:30.986+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:load_data_to_bq
[2024-04-30T18:42:31.009+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:42:31.008+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:load_data_to_bq
[2024-04-30T18:42:31.011+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:42:31.010+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:42:31.045+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:42:31.044+0000] {dag.py:3069} INFO - Creating ORM DAG for load_data_to_bq
[2024-04-30T18:42:31.047+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:42:31.046+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:42:31.114+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.576 seconds
[2024-04-30T18:43:01.315+0000] {processor.py:161} INFO - Started process (PID=420) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:43:01.317+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:43:01.321+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:43:01.320+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:43:01.355+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:43:01.429+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:43:01.428+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:43:01.468+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:43:01.467+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:43:01.581+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.278 seconds
[2024-04-30T18:43:31.652+0000] {processor.py:161} INFO - Started process (PID=451) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:43:31.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:43:31.658+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:43:31.657+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:43:31.692+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:43:31.765+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:43:31.763+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:43:31.806+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:43:31.805+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:43:31.862+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.221 seconds
[2024-04-30T18:44:02.733+0000] {processor.py:161} INFO - Started process (PID=482) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:44:02.736+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:44:02.739+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:44:02.737+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:44:02.773+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:44:02.843+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:44:02.841+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:44:02.883+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:44:02.882+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:44:02.940+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.217 seconds
[2024-04-30T18:44:33.826+0000] {processor.py:161} INFO - Started process (PID=513) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:44:33.830+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:44:33.833+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:44:33.832+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:44:33.872+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:44:33.954+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:44:33.952+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:44:33.993+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:44:33.992+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:44:34.061+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.246 seconds
[2024-04-30T18:45:05.005+0000] {processor.py:161} INFO - Started process (PID=544) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:45:05.008+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:45:05.011+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:45:05.010+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:45:05.046+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:45:05.117+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:45:05.115+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:45:05.158+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:45:05.158+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:45:05.218+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.223 seconds
[2024-04-30T18:45:35.867+0000] {processor.py:161} INFO - Started process (PID=575) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:45:35.870+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:45:35.873+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:45:35.872+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:45:35.907+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:45:35.983+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:45:35.981+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:45:36.029+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:45:36.028+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:45:36.091+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.235 seconds
[2024-04-30T18:45:38.948+0000] {processor.py:161} INFO - Started process (PID=580) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:45:38.952+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:45:38.956+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:45:38.955+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:45:39.013+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:45:39.352+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:45:39.351+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:45:39.389+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:45:39.389+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:45:39.453+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.520 seconds
[2024-04-30T18:46:12.188+0000] {processor.py:161} INFO - Started process (PID=611) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:46:13.056+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:46:13.381+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:46:13.379+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:46:13.911+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:46:14.052+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:46:14.050+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:46:14.102+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:46:14.101+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:46:14.171+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 2.981 seconds
[2024-04-30T18:46:45.136+0000] {processor.py:161} INFO - Started process (PID=648) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:46:45.140+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:46:45.143+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:46:45.141+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:46:45.176+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:46:45.245+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:46:45.243+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:46:45.286+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:46:45.285+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:46:45.342+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.216 seconds
[2024-04-30T18:47:16.117+0000] {processor.py:161} INFO - Started process (PID=679) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:47:16.120+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:47:16.123+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:47:16.122+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:47:16.156+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:47:16.226+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:47:16.224+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:47:16.265+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:47:16.264+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:47:16.320+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.214 seconds
[2024-04-30T18:47:47.164+0000] {processor.py:161} INFO - Started process (PID=710) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:47:47.169+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:47:47.175+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:47:47.173+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:47:47.225+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:47:47.341+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:47:47.339+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:47:47.380+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:47:47.380+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:47:47.438+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.286 seconds
[2024-04-30T18:48:17.586+0000] {processor.py:161} INFO - Started process (PID=741) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:48:17.590+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:48:17.593+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:48:17.591+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:48:17.626+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:48:17.697+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:48:17.695+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:48:17.738+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:48:17.737+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:48:17.794+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.219 seconds
[2024-04-30T18:48:48.782+0000] {processor.py:161} INFO - Started process (PID=772) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:48:48.785+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:48:48.787+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:48:48.786+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:48:48.823+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:48:48.898+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:48:48.897+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:48:48.940+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:48:48.939+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:48:48.996+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.224 seconds
[2024-04-30T18:49:19.387+0000] {processor.py:161} INFO - Started process (PID=803) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:49:19.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:49:19.396+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:49:19.394+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:49:19.436+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:49:19.517+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:49:19.515+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:49:19.563+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:49:19.562+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:49:19.627+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.253 seconds
[2024-04-30T18:49:49.951+0000] {processor.py:161} INFO - Started process (PID=834) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:49:49.954+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:49:49.957+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:49:49.956+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:49:49.996+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:49:50.081+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:49:50.080+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:49:50.128+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:49:50.127+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:49:50.190+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.252 seconds
[2024-04-30T18:50:20.474+0000] {processor.py:161} INFO - Started process (PID=865) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:50:20.478+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:50:20.481+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:50:20.480+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:50:20.520+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:50:20.602+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:50:20.600+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:50:20.651+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:50:20.650+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:50:20.718+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.259 seconds
[2024-04-30T18:50:51.030+0000] {processor.py:161} INFO - Started process (PID=896) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:50:51.034+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:50:51.037+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:50:51.036+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:50:51.076+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:50:51.161+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:50:51.159+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:50:51.206+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:50:51.205+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:50:51.302+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.285 seconds
[2024-04-30T18:51:03.932+0000] {processor.py:161} INFO - Started process (PID=921) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:51:03.936+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:51:03.939+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:51:03.938+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:51:03.978+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:51:04.047+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:51:04.045+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:51:04.086+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:51:04.085+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:51:04.147+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.225 seconds
[2024-04-30T18:51:34.946+0000] {processor.py:161} INFO - Started process (PID=952) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:51:34.950+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:51:34.953+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:51:34.952+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:51:34.988+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:51:35.056+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:51:35.054+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:51:35.095+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:51:35.094+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:51:35.156+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.221 seconds
[2024-04-30T18:52:05.235+0000] {processor.py:161} INFO - Started process (PID=983) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:52:05.239+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:52:05.242+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:52:05.241+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:52:05.277+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:52:05.349+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:52:05.346+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:52:05.388+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:52:05.388+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:52:05.448+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.224 seconds
[2024-04-30T18:52:35.781+0000] {processor.py:161} INFO - Started process (PID=1014) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:52:35.788+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:52:35.792+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:52:35.791+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:52:35.838+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:52:35.921+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:52:35.919+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:52:35.959+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:52:35.958+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:52:36.016+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.258 seconds
[2024-04-30T18:52:36.784+0000] {processor.py:161} INFO - Started process (PID=1019) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:52:36.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:52:36.789+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:52:36.788+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:52:36.830+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:52:37.290+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:52:37.289+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:52:37.323+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:52:37.322+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:52:37.385+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.613 seconds
[2024-04-30T18:53:08.119+0000] {processor.py:161} INFO - Started process (PID=1050) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:53:08.121+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:53:08.124+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:53:08.123+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:53:08.158+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:53:08.229+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:53:08.228+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:53:08.270+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:53:08.269+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:53:08.326+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.217 seconds
[2024-04-30T18:53:38.643+0000] {processor.py:161} INFO - Started process (PID=1081) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:53:38.647+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:53:38.650+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:53:38.649+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:53:38.690+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:53:38.764+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:53:38.762+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:53:38.802+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:53:38.801+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:53:38.857+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.226 seconds
[2024-04-30T18:54:09.140+0000] {processor.py:161} INFO - Started process (PID=1112) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:54:09.144+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:54:09.147+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:54:09.146+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:54:09.182+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:54:09.254+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:54:09.252+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:54:09.292+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:54:09.291+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:54:09.351+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.221 seconds
[2024-04-30T18:54:39.595+0000] {processor.py:161} INFO - Started process (PID=1143) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:54:39.598+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:54:39.601+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:54:39.600+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:54:39.636+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:54:39.706+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:54:39.704+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:54:39.746+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:54:39.746+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:54:39.802+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.218 seconds
[2024-04-30T18:55:10.088+0000] {processor.py:161} INFO - Started process (PID=1174) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:55:10.091+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:55:10.094+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:55:10.092+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:55:10.128+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:55:10.198+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:55:10.196+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:55:10.236+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:55:10.235+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:55:10.298+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.221 seconds
[2024-04-30T18:55:40.571+0000] {processor.py:161} INFO - Started process (PID=1206) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:55:40.575+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:55:40.578+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:55:40.577+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:55:40.612+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:55:40.684+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:55:40.682+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:55:40.722+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:55:40.721+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:55:40.779+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.218 seconds
[2024-04-30T18:56:11.105+0000] {processor.py:161} INFO - Started process (PID=1238) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:56:11.110+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:56:11.115+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:56:11.113+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:56:11.164+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:56:11.234+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:56:11.232+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:56:11.274+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:56:11.273+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:56:11.330+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.243 seconds
[2024-04-30T18:56:41.440+0000] {processor.py:161} INFO - Started process (PID=1269) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:56:41.444+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:56:41.447+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:56:41.446+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:56:41.482+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:56:41.553+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:56:41.551+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:56:41.591+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:56:41.590+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:56:41.648+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.218 seconds
[2024-04-30T18:57:11.788+0000] {processor.py:161} INFO - Started process (PID=1300) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:57:11.792+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:57:11.796+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:57:11.794+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:57:11.846+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:57:11.923+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:57:11.921+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:57:11.964+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:57:11.963+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:57:12.018+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.246 seconds
[2024-04-30T18:57:26.199+0000] {processor.py:161} INFO - Started process (PID=1311) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:57:26.203+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:57:26.206+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:57:26.205+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:57:26.249+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:57:26.581+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:57:26.580+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:57:26.619+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:57:26.619+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:57:26.758+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.569 seconds
[2024-04-30T18:57:57.308+0000] {processor.py:161} INFO - Started process (PID=1341) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:57:57.311+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:57:57.315+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:57:57.313+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:57:57.354+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:57:57.427+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:57:57.424+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:57:57.465+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:57:57.465+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:57:57.522+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.225 seconds
[2024-04-30T18:58:26.259+0000] {processor.py:161} INFO - Started process (PID=1373) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:58:26.263+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:58:26.266+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:58:26.265+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:58:26.308+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:58:26.658+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:58:26.657+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:58:26.695+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:58:26.694+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:58:26.794+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.546 seconds
[2024-04-30T18:58:27.555+0000] {processor.py:161} INFO - Started process (PID=1378) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:58:27.558+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:58:27.562+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:58:27.561+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:58:27.601+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:58:27.638+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:58:27.636+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:58:27.678+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:58:27.677+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:58:27.738+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.201 seconds
[2024-04-30T18:58:58.255+0000] {processor.py:161} INFO - Started process (PID=1421) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:58:58.258+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:58:58.261+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:58:58.260+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:58:58.295+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:58:58.369+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:58:58.366+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:58:58.408+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:58:58.407+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:58:58.465+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.221 seconds
[2024-04-30T18:59:28.597+0000] {processor.py:161} INFO - Started process (PID=1452) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:59:28.601+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:59:28.604+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:59:28.603+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:59:28.638+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:59:28.706+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:59:28.705+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:59:28.745+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:59:28.744+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:59:28.801+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.214 seconds
[2024-04-30T18:59:59.624+0000] {processor.py:161} INFO - Started process (PID=1483) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T18:59:59.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T18:59:59.630+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:59:59.629+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:59:59.666+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T18:59:59.765+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:59:59.762+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T18:59:59.828+0000] {logging_mixin.py:188} INFO - [2024-04-30T18:59:59.827+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T18:59:59.885+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.271 seconds
[2024-04-30T19:00:30.671+0000] {processor.py:161} INFO - Started process (PID=1514) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:00:30.675+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:00:30.678+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:00:30.677+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:00:30.713+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:00:30.783+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:00:30.781+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:00:30.822+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:00:30.821+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:00:30.877+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.216 seconds
[2024-04-30T19:00:54.613+0000] {processor.py:161} INFO - Started process (PID=1545) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:00:54.616+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:00:54.619+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:00:54.618+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:00:54.660+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:00:54.776+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:00:54.773+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:00:54.836+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:00:54.836+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:00:54.898+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.295 seconds
[2024-04-30T19:01:25.756+0000] {processor.py:161} INFO - Started process (PID=1588) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:01:25.759+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:01:25.762+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:01:25.761+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:01:25.795+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:01:25.865+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:01:25.863+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:01:25.904+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:01:25.903+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:01:25.962+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.216 seconds
[2024-04-30T19:01:53.745+0000] {processor.py:161} INFO - Started process (PID=1619) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:01:53.749+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:01:53.753+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:01:53.750+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:01:53.799+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:01:54.235+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:01:54.233+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:01:54.271+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:01:54.270+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:01:54.335+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.601 seconds
[2024-04-30T19:02:24.439+0000] {processor.py:161} INFO - Started process (PID=1662) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:02:24.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:02:24.447+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:02:24.445+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:02:24.482+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:02:24.551+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:02:24.549+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:02:24.589+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:02:24.588+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:02:24.644+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.216 seconds
[2024-04-30T19:02:55.471+0000] {processor.py:161} INFO - Started process (PID=1693) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:02:55.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:02:55.477+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:02:55.476+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:02:55.511+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:02:55.580+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:02:55.578+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:02:55.618+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:02:55.617+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:02:55.677+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.216 seconds
[2024-04-30T19:02:56.804+0000] {processor.py:161} INFO - Started process (PID=1698) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:02:56.809+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:02:56.815+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:02:56.813+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:02:56.865+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:02:56.938+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:02:56.936+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:02:56.985+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:02:56.984+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:02:57.049+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.265 seconds
[2024-04-30T19:03:27.235+0000] {processor.py:161} INFO - Started process (PID=1731) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:03:27.239+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:03:27.245+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:03:27.243+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:03:27.302+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:03:27.409+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:03:27.407+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:03:27.466+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:03:27.465+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:03:27.533+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.320 seconds
[2024-04-30T19:03:57.709+0000] {processor.py:161} INFO - Started process (PID=1772) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:03:57.711+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:03:57.714+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:03:57.713+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:03:57.749+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:03:57.820+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:03:57.818+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:03:57.859+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:03:57.858+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:03:57.915+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.217 seconds
[2024-04-30T19:04:28.486+0000] {processor.py:161} INFO - Started process (PID=1803) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:04:28.490+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:04:28.493+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:04:28.492+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:04:28.528+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:04:28.600+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:04:28.597+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:04:28.641+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:04:28.640+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:04:28.698+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.223 seconds
[2024-04-30T19:04:30.570+0000] {processor.py:161} INFO - Started process (PID=1808) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:04:30.574+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:04:30.578+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:04:30.577+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:04:30.604+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:04:30.600+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import GCSToBigQueryOperator
ImportError: cannot import name 'GCSToBigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-04-30T19:04:30.606+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:04:30.684+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.131 seconds
[2024-04-30T19:04:31.576+0000] {processor.py:161} INFO - Started process (PID=1813) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:04:31.580+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:04:31.583+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:04:31.582+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:04:31.593+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:04:31.591+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import GCSToBigQueryOperator
ImportError: cannot import name 'GCSToBigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-04-30T19:04:31.595+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:04:31.652+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.087 seconds
[2024-04-30T19:04:59.531+0000] {processor.py:161} INFO - Started process (PID=1844) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:04:59.534+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:04:59.537+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:04:59.536+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:04:59.553+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:04:59.551+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import GCSToBigQueryOperator
ImportError: cannot import name 'GCSToBigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-04-30T19:04:59.554+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:04:59.606+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.086 seconds
[2024-04-30T19:05:30.502+0000] {processor.py:161} INFO - Started process (PID=1875) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:05:30.505+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:05:30.508+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:05:30.507+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:05:30.518+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:05:30.516+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import GCSToBigQueryOperator
ImportError: cannot import name 'GCSToBigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-04-30T19:05:30.520+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:05:30.577+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.084 seconds
[2024-04-30T19:06:01.610+0000] {processor.py:161} INFO - Started process (PID=1906) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:06:01.616+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:06:01.622+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:06:01.621+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:06:01.643+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:06:01.632+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import GCSToBigQueryOperator
ImportError: cannot import name 'GCSToBigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-04-30T19:06:01.645+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:06:01.712+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T19:06:32.691+0000] {processor.py:161} INFO - Started process (PID=1937) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:06:32.693+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:06:32.696+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:06:32.695+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:06:32.713+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:06:32.711+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 11, in <module>
    from airflow.contrib.operators.gcs_to_bigquery import GCSToBigQueryOperator
ModuleNotFoundError: No module named 'airflow.contrib.operators.gcs_to_bigquery'
[2024-04-30T19:06:32.714+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:06:32.767+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.086 seconds
[2024-04-30T19:06:55.362+0000] {processor.py:161} INFO - Started process (PID=1967) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:06:55.366+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:06:55.369+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:06:55.368+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:06:55.410+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:06:55.739+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:06:55.738+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:06:55.776+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:06:55.776+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:06:55.841+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.488 seconds
[2024-04-30T19:07:26.417+0000] {processor.py:161} INFO - Started process (PID=1998) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:07:26.420+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:07:26.423+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:07:26.422+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:07:26.473+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:07:26.546+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:07:26.546+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:07:26.584+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:07:26.584+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:07:26.638+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.231 seconds
[2024-04-30T19:07:57.430+0000] {processor.py:161} INFO - Started process (PID=2028) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:07:57.434+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:07:57.437+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:07:57.436+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:07:57.470+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:07:57.538+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:07:57.537+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:07:57.577+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:07:57.576+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:07:57.633+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.212 seconds
[2024-04-30T19:08:28.513+0000] {processor.py:161} INFO - Started process (PID=2059) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:08:28.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:08:28.522+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:08:28.521+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:08:28.559+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:08:28.631+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:08:28.630+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:08:28.673+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:08:28.673+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:08:28.729+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.238 seconds
[2024-04-30T19:08:59.616+0000] {processor.py:161} INFO - Started process (PID=2090) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:08:59.620+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:08:59.623+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:08:59.622+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:08:59.657+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:08:59.725+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:08:59.724+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:08:59.764+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:08:59.763+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:08:59.823+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.217 seconds
[2024-04-30T19:09:30.817+0000] {processor.py:161} INFO - Started process (PID=2122) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:09:30.820+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:09:30.823+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:09:30.822+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:09:30.858+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:09:30.950+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:09:30.950+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:09:30.994+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:09:30.993+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:09:31.051+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.244 seconds
[2024-04-30T19:10:01.874+0000] {processor.py:161} INFO - Started process (PID=2153) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:10:01.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:10:01.888+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:10:01.886+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:10:01.922+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:10:01.990+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:10:01.989+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:10:02.030+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:10:02.030+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:10:02.087+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.234 seconds
[2024-04-30T19:10:32.846+0000] {processor.py:161} INFO - Started process (PID=2184) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:10:32.848+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:10:32.851+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:10:32.850+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:10:32.885+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:10:32.953+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:10:32.952+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:10:32.992+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:10:32.991+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:10:33.049+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.214 seconds
[2024-04-30T19:11:03.882+0000] {processor.py:161} INFO - Started process (PID=2215) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:11:03.886+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:11:03.890+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:11:03.889+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:11:03.924+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:11:03.992+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:11:03.992+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:11:04.033+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:11:04.032+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:11:04.090+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.219 seconds
[2024-04-30T19:11:34.993+0000] {processor.py:161} INFO - Started process (PID=2246) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:11:34.997+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:11:35.000+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:11:34.998+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:11:35.033+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:11:35.103+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:11:35.102+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:11:35.143+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:11:35.142+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:11:35.199+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.215 seconds
[2024-04-30T19:12:06.021+0000] {processor.py:161} INFO - Started process (PID=2277) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:12:06.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:12:06.028+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:12:06.026+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:12:06.061+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:12:06.128+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:12:06.127+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:12:06.170+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:12:06.169+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:12:06.233+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.221 seconds
[2024-04-30T19:12:37.126+0000] {processor.py:161} INFO - Started process (PID=2308) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:12:37.128+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:12:37.131+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:12:37.130+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:12:37.165+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:12:37.233+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:12:37.232+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:12:37.272+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:12:37.272+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:12:37.331+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.215 seconds
[2024-04-30T19:13:08.178+0000] {processor.py:161} INFO - Started process (PID=2339) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:13:08.181+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:13:08.184+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:13:08.183+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:13:08.218+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:13:08.285+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:13:08.284+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:13:08.325+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:13:08.325+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:13:08.390+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.221 seconds
[2024-04-30T19:13:39.222+0000] {processor.py:161} INFO - Started process (PID=2370) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:13:39.225+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:13:39.228+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:13:39.227+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:13:39.261+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:13:39.329+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:13:39.328+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:13:39.368+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:13:39.367+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:13:39.425+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.215 seconds
[2024-04-30T19:14:10.282+0000] {processor.py:161} INFO - Started process (PID=2401) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:14:10.285+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:14:10.288+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:14:10.287+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:14:10.322+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:14:10.391+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:14:10.390+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:14:10.430+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:14:10.429+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:14:10.485+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.213 seconds
[2024-04-30T19:14:41.331+0000] {processor.py:161} INFO - Started process (PID=2433) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:14:41.335+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:14:41.338+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:14:41.337+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:14:41.377+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:14:41.453+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:14:41.452+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:14:41.533+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:14:41.532+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:14:41.599+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.281 seconds
[2024-04-30T19:15:12.306+0000] {processor.py:161} INFO - Started process (PID=2468) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:15:12.309+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:15:12.312+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:15:12.311+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:15:12.346+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:15:12.414+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:15:12.413+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:15:12.452+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:15:12.451+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:15:12.508+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.211 seconds
[2024-04-30T19:15:43.390+0000] {processor.py:161} INFO - Started process (PID=2499) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:15:43.393+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:15:43.396+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:15:43.395+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:15:43.431+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:15:43.498+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:15:43.497+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:15:43.537+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:15:43.536+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:15:43.593+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.212 seconds
[2024-04-30T19:16:14.458+0000] {processor.py:161} INFO - Started process (PID=2530) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:16:14.461+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:16:14.464+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:16:14.463+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:16:14.498+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:16:14.565+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:16:14.565+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:16:14.618+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:16:14.617+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:16:14.677+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.228 seconds
[2024-04-30T19:16:45.440+0000] {processor.py:161} INFO - Started process (PID=2561) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:16:45.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:16:45.446+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:16:45.445+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:16:45.480+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:16:45.547+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:16:45.546+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:16:45.586+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:16:45.585+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:16:45.641+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.211 seconds
[2024-04-30T19:17:16.470+0000] {processor.py:161} INFO - Started process (PID=2592) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:17:16.473+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:17:16.476+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:17:16.475+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:17:16.510+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:17:16.576+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:17:16.576+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:17:16.615+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:17:16.614+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:17:16.674+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.215 seconds
[2024-04-30T19:17:47.524+0000] {processor.py:161} INFO - Started process (PID=2623) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:17:47.528+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:17:47.531+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:17:47.530+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:17:47.565+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:17:47.633+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:17:47.632+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:17:47.674+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:17:47.673+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:17:47.730+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.216 seconds
[2024-04-30T19:18:18.512+0000] {processor.py:161} INFO - Started process (PID=2654) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:18:18.515+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:18:18.518+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:18:18.517+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:18:18.552+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:18:18.619+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:18:18.618+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:18:18.668+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:18:18.667+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:18:18.768+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.265 seconds
[2024-04-30T19:18:49.502+0000] {processor.py:161} INFO - Started process (PID=2685) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:18:49.506+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:18:49.509+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:18:49.508+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:18:49.552+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:18:49.627+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:18:49.626+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:18:49.667+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:18:49.666+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:18:49.722+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.231 seconds
[2024-04-30T19:19:20.531+0000] {processor.py:161} INFO - Started process (PID=2716) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:19:20.535+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:19:20.538+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:19:20.537+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:19:20.581+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:19:20.658+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:19:20.657+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:19:20.699+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:19:20.698+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:19:20.753+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.234 seconds
[2024-04-30T19:19:51.708+0000] {processor.py:161} INFO - Started process (PID=2747) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:19:51.712+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:19:51.715+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:19:51.714+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:19:51.754+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:19:51.829+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:19:51.829+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:19:51.872+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:19:51.871+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:19:51.927+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.230 seconds
[2024-04-30T19:20:22.700+0000] {processor.py:161} INFO - Started process (PID=2778) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:20:22.704+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:20:22.707+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:20:22.706+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:20:22.741+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:20:22.808+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:20:22.807+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:20:22.848+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:20:22.847+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:20:22.905+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.216 seconds
[2024-04-30T19:20:53.826+0000] {processor.py:161} INFO - Started process (PID=2809) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:20:53.829+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:20:53.832+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:20:53.831+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:20:53.866+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:20:53.934+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:20:53.934+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:20:53.975+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:20:53.974+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:20:54.031+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.215 seconds
[2024-04-30T19:21:24.889+0000] {processor.py:161} INFO - Started process (PID=2840) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:21:24.893+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:21:24.897+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:21:24.896+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:21:24.934+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:21:25.004+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:21:25.004+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:21:25.045+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:21:25.044+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:21:25.139+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.261 seconds
[2024-04-30T19:21:55.914+0000] {processor.py:161} INFO - Started process (PID=2878) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:21:55.918+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:21:55.921+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:21:55.920+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:21:55.962+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:21:56.043+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:21:56.042+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:21:56.098+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:21:56.097+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:21:56.203+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.301 seconds
[2024-04-30T19:22:26.969+0000] {processor.py:161} INFO - Started process (PID=2909) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:22:26.974+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:22:26.977+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:22:26.976+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:22:27.018+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:22:27.102+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:22:27.101+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:22:27.141+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:22:27.140+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:22:27.196+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.238 seconds
[2024-04-30T19:22:57.979+0000] {processor.py:161} INFO - Started process (PID=2941) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:22:57.982+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:22:57.985+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:22:57.984+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:22:58.019+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:22:58.087+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:22:58.087+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:22:58.127+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:22:58.127+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:22:58.183+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.214 seconds
[2024-04-30T19:23:29.058+0000] {processor.py:161} INFO - Started process (PID=2972) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:23:29.061+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:23:29.064+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:23:29.063+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:23:29.098+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:23:29.165+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:23:29.165+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:23:29.205+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:23:29.205+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:23:29.268+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.219 seconds
[2024-04-30T19:24:00.148+0000] {processor.py:161} INFO - Started process (PID=3003) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:24:00.152+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:24:00.155+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:24:00.154+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:24:00.188+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:24:00.256+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:24:00.255+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:24:00.295+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:24:00.294+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:24:00.350+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.211 seconds
[2024-04-30T19:24:31.281+0000] {processor.py:161} INFO - Started process (PID=3034) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:24:31.284+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:24:31.288+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:24:31.286+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:24:31.346+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:24:31.446+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:24:31.445+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:24:31.502+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:24:31.501+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:24:31.561+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.291 seconds
[2024-04-30T19:25:02.275+0000] {processor.py:161} INFO - Started process (PID=3065) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:25:02.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:25:02.281+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:25:02.280+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:25:02.315+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:25:02.383+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:25:02.383+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:25:02.422+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:25:02.422+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:25:02.481+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.216 seconds
[2024-04-30T19:25:33.290+0000] {processor.py:161} INFO - Started process (PID=3096) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:25:33.293+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:25:33.296+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:25:33.295+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:25:33.330+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:25:33.398+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:25:33.397+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:25:33.437+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:25:33.436+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:25:33.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.237 seconds
[2024-04-30T19:26:04.386+0000] {processor.py:161} INFO - Started process (PID=3127) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:26:04.390+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:26:04.393+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:26:04.392+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:26:04.426+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:26:04.494+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:26:04.493+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:26:04.534+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:26:04.533+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:26:04.588+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.212 seconds
[2024-04-30T19:26:35.426+0000] {processor.py:161} INFO - Started process (PID=3158) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:26:35.429+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:26:35.432+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:26:35.431+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:26:35.471+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:26:35.549+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:26:35.548+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:26:35.587+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:26:35.587+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:26:35.645+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.229 seconds
[2024-04-30T19:27:06.419+0000] {processor.py:161} INFO - Started process (PID=3189) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:27:06.424+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:27:06.427+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:27:06.426+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:27:06.462+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:27:06.532+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:27:06.532+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:27:06.571+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:27:06.570+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:27:06.628+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.219 seconds
[2024-04-30T19:27:37.517+0000] {processor.py:161} INFO - Started process (PID=3220) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:27:37.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:27:37.524+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:27:37.523+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:27:37.559+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:27:37.627+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:27:37.626+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:27:37.667+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:27:37.666+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:27:37.724+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.216 seconds
[2024-04-30T19:28:08.619+0000] {processor.py:161} INFO - Started process (PID=3251) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:28:08.623+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:28:08.626+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:28:08.625+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:28:08.660+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:28:08.728+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:28:08.727+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:28:08.766+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:28:08.766+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:28:08.825+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.218 seconds
[2024-04-30T19:28:39.748+0000] {processor.py:161} INFO - Started process (PID=3282) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:28:39.751+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:28:39.754+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:28:39.753+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:28:39.787+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:28:39.858+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:28:39.857+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:28:39.897+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:28:39.896+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:28:39.960+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.224 seconds
[2024-04-30T19:29:10.869+0000] {processor.py:161} INFO - Started process (PID=3313) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:29:10.872+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:29:10.875+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:29:10.874+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:29:10.908+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:29:10.977+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:29:10.976+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:29:11.016+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:29:11.015+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:29:11.071+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.212 seconds
[2024-04-30T19:29:41.934+0000] {processor.py:161} INFO - Started process (PID=3344) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:29:41.937+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:29:41.940+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:29:41.939+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:29:41.974+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:29:42.042+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:29:42.042+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:29:42.082+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:29:42.081+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:29:42.141+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.217 seconds
[2024-04-30T19:30:12.971+0000] {processor.py:161} INFO - Started process (PID=3375) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:30:12.975+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:30:12.978+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:30:12.977+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:30:13.012+0000] {processor.py:840} INFO - DAG(s) 'load_data_to_bq' retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:30:13.082+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:30:13.081+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-04-30T19:30:13.121+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:30:13.120+0000] {dag.py:3834} INFO - Setting next_dagrun for load_data_to_bq to None, run_after=None
[2024-04-30T19:30:13.178+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.218 seconds
[2024-04-30T19:30:17.044+0000] {processor.py:161} INFO - Started process (PID=3380) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:30:17.048+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:30:17.051+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:30:17.050+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:30:17.080+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:30:17.078+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 177, in <module>
    load_csv_to_bq_task = _load_csv_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_csv_to_bq
    load_task = GCSToBigQueryOperator(
NameError: name 'GCSToBigQueryOperator' is not defined
[2024-04-30T19:30:17.081+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:30:17.138+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.105 seconds
[2024-04-30T19:30:43.078+0000] {processor.py:161} INFO - Started process (PID=3411) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:30:43.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:30:43.084+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:30:43.083+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:30:43.112+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:30:43.110+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 177, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
NameError: name 'GCSToBigQueryOperator' is not defined
[2024-04-30T19:30:43.113+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:30:43.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T19:31:14.135+0000] {processor.py:161} INFO - Started process (PID=3442) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:31:14.138+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:31:14.142+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:31:14.141+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:31:14.164+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:31:14.162+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 177, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
NameError: name 'GCSToBigQueryOperator' is not defined
[2024-04-30T19:31:14.166+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:31:14.217+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T19:31:43.302+0000] {processor.py:161} INFO - Started process (PID=3474) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:31:43.305+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:31:43.308+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:31:43.307+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:31:43.340+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:31:43.337+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 177, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 420, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'bucket'
[2024-04-30T19:31:43.342+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:31:43.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.101 seconds
[2024-04-30T19:32:14.378+0000] {processor.py:161} INFO - Started process (PID=3505) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:32:14.382+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:32:14.387+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:32:14.385+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:32:14.418+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:32:14.414+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 177, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 420, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'bucket'
[2024-04-30T19:32:14.419+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:32:14.477+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.112 seconds
[2024-04-30T19:32:45.463+0000] {processor.py:161} INFO - Started process (PID=3537) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:32:45.466+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:32:45.469+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:32:45.468+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:32:45.492+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:32:45.489+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 177, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 420, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'bucket'
[2024-04-30T19:32:45.494+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:32:45.547+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T19:33:16.457+0000] {processor.py:161} INFO - Started process (PID=3568) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:33:16.461+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:33:16.464+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:33:16.463+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:33:16.489+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:33:16.484+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 177, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 420, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'bucket'
[2024-04-30T19:33:16.490+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:33:16.547+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.102 seconds
[2024-04-30T19:33:47.533+0000] {processor.py:161} INFO - Started process (PID=3599) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:33:47.536+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:33:47.539+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:33:47.538+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:33:47.563+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:33:47.559+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 177, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 420, in apply_defaults
    raise AirflowException(f"missing keyword argument {missing_args.pop()!r}")
airflow.exceptions.AirflowException: missing keyword argument 'bucket'
[2024-04-30T19:33:47.564+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:33:47.615+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T19:33:51.603+0000] {processor.py:161} INFO - Started process (PID=3604) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:33:51.607+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:33:51.612+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:33:51.610+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:33:51.645+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:33:51.642+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 120, in _load_data_to_bq
    bucket=bucket_name,
NameError: name 'bucket_name' is not defined
[2024-04-30T19:33:51.647+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:33:51.711+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.122 seconds
[2024-04-30T19:34:22.764+0000] {processor.py:161} INFO - Started process (PID=3635) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:34:22.768+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:34:22.771+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:34:22.770+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:34:22.792+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:34:22.789+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 120, in _load_data_to_bq
    bucket=bucket_name,
NameError: name 'bucket_name' is not defined
[2024-04-30T19:34:22.793+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:34:22.845+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.091 seconds
[2024-04-30T19:34:37.982+0000] {processor.py:161} INFO - Started process (PID=3646) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:34:37.985+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:34:37.988+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:34:37.987+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:34:38.018+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:34:38.014+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_csv_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:34:38.019+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:34:38.070+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.101 seconds
[2024-04-30T19:35:08.860+0000] {processor.py:161} INFO - Started process (PID=3677) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:35:08.864+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:35:08.867+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:35:08.866+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:35:08.892+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:35:08.887+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_csv_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:35:08.893+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:35:08.946+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T19:35:14.962+0000] {processor.py:161} INFO - Started process (PID=3682) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:35:14.966+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:35:14.971+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:35:14.969+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:35:15.020+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:35:15.013+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:35:15.022+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:35:15.088+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.143 seconds
[2024-04-30T19:35:46.011+0000] {processor.py:161} INFO - Started process (PID=3714) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:35:46.016+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:35:46.020+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:35:46.018+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:35:46.048+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:35:46.043+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:35:46.049+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:35:46.132+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.136 seconds
[2024-04-30T19:36:17.031+0000] {processor.py:161} INFO - Started process (PID=3745) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:36:17.035+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:36:17.040+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:36:17.038+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:36:17.069+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:36:17.064+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:36:17.071+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:36:17.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.118 seconds
[2024-04-30T19:36:48.107+0000] {processor.py:161} INFO - Started process (PID=3776) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:36:48.123+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:36:48.128+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:36:48.126+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:36:48.161+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:36:48.155+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:36:48.163+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:36:48.244+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.163 seconds
[2024-04-30T19:37:19.181+0000] {processor.py:161} INFO - Started process (PID=3807) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:37:19.185+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:37:19.190+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:37:19.188+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:37:19.218+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:37:19.213+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:37:19.220+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:37:19.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T19:37:50.184+0000] {processor.py:161} INFO - Started process (PID=3838) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:37:50.188+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:37:50.192+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:37:50.190+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:37:50.219+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:37:50.214+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:37:50.221+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:37:50.285+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T19:38:21.184+0000] {processor.py:161} INFO - Started process (PID=3870) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:38:21.188+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:38:21.192+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:38:21.190+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:38:21.220+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:38:21.215+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:38:21.221+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:38:21.284+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T19:38:52.010+0000] {processor.py:161} INFO - Started process (PID=3902) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:38:52.013+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:38:52.017+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:38:52.015+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:38:52.045+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:38:52.040+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:38:52.047+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:38:52.108+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.110 seconds
[2024-04-30T19:39:23.032+0000] {processor.py:161} INFO - Started process (PID=3933) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:39:23.037+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:39:23.041+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:39:23.040+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:39:23.070+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:39:23.065+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:39:23.072+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:39:23.136+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.120 seconds
[2024-04-30T19:39:54.104+0000] {processor.py:161} INFO - Started process (PID=3964) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:39:54.108+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:39:54.112+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:39:54.110+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:39:54.139+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:39:54.134+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:39:54.140+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:39:54.202+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T19:40:25.173+0000] {processor.py:161} INFO - Started process (PID=3995) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:40:25.177+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:40:25.181+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:40:25.179+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:40:25.208+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:40:25.203+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:40:25.209+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:40:25.273+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T19:40:56.189+0000] {processor.py:161} INFO - Started process (PID=4026) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:40:56.194+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:40:56.197+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:40:56.196+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:40:56.225+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:40:56.219+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:40:56.226+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:40:56.288+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T19:41:27.165+0000] {processor.py:161} INFO - Started process (PID=4057) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:41:27.170+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:41:27.174+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:41:27.173+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:41:27.203+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:41:27.198+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:41:27.205+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:41:27.270+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.120 seconds
[2024-04-30T19:41:57.959+0000] {processor.py:161} INFO - Started process (PID=4088) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:41:57.963+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:41:57.966+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:41:57.965+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:41:57.994+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:41:57.989+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:41:57.995+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:41:58.062+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T19:42:28.975+0000] {processor.py:161} INFO - Started process (PID=4119) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:42:28.980+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:42:28.984+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:42:28.983+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:42:29.012+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:42:29.007+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:42:29.013+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:42:29.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T19:43:00.003+0000] {processor.py:161} INFO - Started process (PID=4150) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:43:00.007+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:43:00.011+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:43:00.009+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:43:00.039+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:43:00.034+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:43:00.041+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:43:00.105+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T19:43:30.998+0000] {processor.py:161} INFO - Started process (PID=4181) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:43:31.002+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:43:31.006+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:43:31.004+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:43:31.034+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:43:31.029+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:43:31.036+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:43:31.099+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T19:44:02.084+0000] {processor.py:161} INFO - Started process (PID=4212) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:44:02.089+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:44:02.093+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:44:02.091+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:44:02.120+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:44:02.115+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:44:02.122+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:44:02.186+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T19:44:33.204+0000] {processor.py:161} INFO - Started process (PID=4249) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:44:33.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:44:33.213+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:44:33.211+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:44:33.240+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:44:33.235+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:44:33.242+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:44:33.305+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T19:45:04.280+0000] {processor.py:161} INFO - Started process (PID=4280) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:45:04.284+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:45:04.288+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:45:04.287+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:45:04.316+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:45:04.311+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:45:04.317+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:45:04.379+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T19:45:35.325+0000] {processor.py:161} INFO - Started process (PID=4311) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:45:35.331+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:45:35.341+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:45:35.340+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:45:35.371+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:45:35.366+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:45:35.373+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:45:35.435+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.124 seconds
[2024-04-30T19:46:06.319+0000] {processor.py:161} INFO - Started process (PID=4342) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:46:06.324+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:46:06.328+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:46:06.326+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:46:06.355+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:46:06.350+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:46:06.357+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:46:06.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.123 seconds
[2024-04-30T19:46:37.317+0000] {processor.py:161} INFO - Started process (PID=4373) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:46:37.321+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:46:37.325+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:46:37.323+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:46:37.352+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:46:37.347+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:46:37.354+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:46:37.416+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T19:47:08.319+0000] {processor.py:161} INFO - Started process (PID=4404) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:47:08.323+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:47:08.327+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:47:08.326+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:47:08.355+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:47:08.350+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:47:08.356+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:47:08.422+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.118 seconds
[2024-04-30T19:47:39.322+0000] {processor.py:161} INFO - Started process (PID=4435) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:47:39.327+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:47:39.331+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:47:39.330+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:47:39.359+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:47:39.354+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:47:39.361+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:47:39.437+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.129 seconds
[2024-04-30T19:48:10.288+0000] {processor.py:161} INFO - Started process (PID=4466) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:48:10.293+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:48:10.296+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:48:10.295+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:48:10.323+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:48:10.318+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:48:10.325+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:48:10.388+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T19:48:41.383+0000] {processor.py:161} INFO - Started process (PID=4497) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:48:41.387+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:48:41.391+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:48:41.389+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:48:41.419+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:48:41.413+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:48:41.420+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:48:41.483+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T19:49:12.547+0000] {processor.py:161} INFO - Started process (PID=4529) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:49:12.551+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:49:12.555+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:49:12.553+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:49:12.583+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:49:12.578+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:49:12.584+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:49:12.648+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T19:49:43.630+0000] {processor.py:161} INFO - Started process (PID=4560) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:49:43.634+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:49:43.640+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:49:43.639+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:49:43.669+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:49:43.664+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:49:43.671+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:49:43.734+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.118 seconds
[2024-04-30T19:50:14.609+0000] {processor.py:161} INFO - Started process (PID=4591) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:50:14.614+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:50:14.618+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:50:14.616+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:50:14.645+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:50:14.640+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:50:14.646+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:50:14.709+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T19:50:45.133+0000] {processor.py:161} INFO - Started process (PID=4622) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:50:45.136+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:50:45.139+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:50:45.138+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:50:45.162+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:50:45.158+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:50:45.164+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:50:45.215+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.091 seconds
[2024-04-30T19:51:15.559+0000] {processor.py:161} INFO - Started process (PID=4653) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:51:15.563+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:51:15.566+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:51:15.565+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:51:15.594+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:51:15.589+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:51:15.595+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:51:15.655+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.107 seconds
[2024-04-30T19:51:45.868+0000] {processor.py:161} INFO - Started process (PID=4685) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:51:45.872+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:51:45.875+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:51:45.874+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:51:45.899+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:51:45.895+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:51:45.901+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:51:45.955+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T19:52:16.227+0000] {processor.py:161} INFO - Started process (PID=4716) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:52:16.229+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:52:16.232+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:52:16.231+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:52:16.256+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:52:16.251+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:52:16.257+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:52:16.311+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T19:52:46.545+0000] {processor.py:161} INFO - Started process (PID=4747) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:52:46.549+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:52:46.551+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:52:46.550+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:52:46.578+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:52:46.573+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:52:46.579+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:52:46.640+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.105 seconds
[2024-04-30T19:53:17.031+0000] {processor.py:161} INFO - Started process (PID=4778) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:53:17.035+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:53:17.038+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:53:17.036+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:53:17.062+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:53:17.057+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:53:17.063+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:53:17.117+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T19:53:47.398+0000] {processor.py:161} INFO - Started process (PID=4809) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:53:47.402+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:53:47.405+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:53:47.404+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:53:47.431+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:53:47.426+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:53:47.433+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:53:47.483+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T19:54:18.352+0000] {processor.py:161} INFO - Started process (PID=4840) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:54:18.355+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:54:18.358+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:54:18.357+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:54:18.382+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:54:18.377+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:54:18.383+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:54:18.434+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T19:54:49.392+0000] {processor.py:161} INFO - Started process (PID=4871) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:54:49.394+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:54:49.397+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:54:49.396+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:54:49.421+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:54:49.417+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:54:49.422+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:54:49.475+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T19:55:19.554+0000] {processor.py:161} INFO - Started process (PID=4902) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:55:19.558+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:55:19.561+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:55:19.560+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:55:19.585+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:55:19.581+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:55:19.586+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:55:19.651+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.107 seconds
[2024-04-30T19:55:49.887+0000] {processor.py:161} INFO - Started process (PID=4933) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:55:49.894+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:55:49.897+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:55:49.896+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:55:49.921+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:55:49.917+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:55:49.923+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:55:49.980+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.103 seconds
[2024-04-30T19:56:20.351+0000] {processor.py:161} INFO - Started process (PID=4964) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:56:20.399+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:56:20.402+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:56:20.401+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:56:20.427+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:56:20.422+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:56:20.428+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:56:20.485+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.144 seconds
[2024-04-30T19:56:50.697+0000] {processor.py:161} INFO - Started process (PID=4995) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:56:50.701+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:56:50.703+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:56:50.702+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:56:50.729+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:56:50.723+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:56:50.730+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:56:50.802+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T19:57:21.516+0000] {processor.py:161} INFO - Started process (PID=5026) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:57:21.520+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:57:21.523+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:57:21.522+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:57:21.547+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:57:21.543+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:57:21.549+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:57:21.602+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T19:57:52.328+0000] {processor.py:161} INFO - Started process (PID=5057) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:57:52.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:57:52.334+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:57:52.333+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:57:52.359+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:57:52.354+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:57:52.360+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:57:52.416+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.099 seconds
[2024-04-30T19:58:22.776+0000] {processor.py:161} INFO - Started process (PID=5088) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:58:22.781+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:58:22.788+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:58:22.785+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:58:22.837+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:58:22.832+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:58:22.838+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:58:22.900+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.147 seconds
[2024-04-30T19:58:53.655+0000] {processor.py:161} INFO - Started process (PID=5119) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:58:53.659+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:58:53.662+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:58:53.661+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:58:53.686+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:58:53.682+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:58:53.688+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:58:53.741+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T19:59:24.459+0000] {processor.py:161} INFO - Started process (PID=5150) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:59:24.462+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:59:24.465+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:59:24.464+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:59:24.490+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:59:24.486+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:59:24.491+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:59:24.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T19:59:54.883+0000] {processor.py:161} INFO - Started process (PID=5181) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T19:59:54.886+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T19:59:54.889+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:59:54.888+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:59:54.913+0000] {logging_mixin.py:188} INFO - [2024-04-30T19:59:54.909+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T19:59:54.914+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T19:59:54.968+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:00:25.660+0000] {processor.py:161} INFO - Started process (PID=5212) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:00:25.663+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:00:25.666+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:00:25.665+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:00:25.691+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:00:25.686+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:00:25.692+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:00:25.745+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:00:56.472+0000] {processor.py:161} INFO - Started process (PID=5243) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:00:56.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:00:56.478+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:00:56.477+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:00:56.513+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:00:56.507+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:00:56.514+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:00:56.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T20:01:26.857+0000] {processor.py:161} INFO - Started process (PID=5274) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:01:26.861+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:01:26.864+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:01:26.862+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:01:26.888+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:01:26.883+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:01:26.889+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:01:26.943+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:01:57.780+0000] {processor.py:161} INFO - Started process (PID=5305) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:01:57.783+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:01:57.786+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:01:57.785+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:01:57.810+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:01:57.805+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:01:57.811+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:01:57.863+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:02:28.804+0000] {processor.py:161} INFO - Started process (PID=5336) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:02:28.808+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:02:28.812+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:02:28.811+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:02:28.847+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:02:28.841+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:02:28.849+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:02:28.906+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T20:02:59.038+0000] {processor.py:161} INFO - Started process (PID=5367) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:02:59.042+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:02:59.045+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:02:59.044+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:02:59.070+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:02:59.065+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:02:59.071+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:02:59.131+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.104 seconds
[2024-04-30T20:03:30.153+0000] {processor.py:161} INFO - Started process (PID=5399) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:03:30.156+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:03:30.159+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:03:30.158+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:03:30.183+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:03:30.178+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:03:30.184+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:03:30.235+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T20:04:01.113+0000] {processor.py:161} INFO - Started process (PID=5430) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:04:01.117+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:04:01.120+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:04:01.119+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:04:01.145+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:04:01.141+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:04:01.146+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:04:01.197+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T20:04:31.479+0000] {processor.py:161} INFO - Started process (PID=5461) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:04:31.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:04:31.486+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:04:31.485+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:04:31.510+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:04:31.506+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:04:31.512+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:04:31.563+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:05:02.484+0000] {processor.py:161} INFO - Started process (PID=5493) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:05:02.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:05:02.492+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:05:02.490+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:05:02.518+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:05:02.513+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:05:02.520+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:05:02.573+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.100 seconds
[2024-04-30T20:05:33.545+0000] {processor.py:161} INFO - Started process (PID=5524) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:05:33.549+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:05:33.551+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:05:33.550+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:05:33.575+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:05:33.571+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:05:33.577+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:05:33.629+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:06:03.770+0000] {processor.py:161} INFO - Started process (PID=5555) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:06:03.773+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:06:03.776+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:06:03.775+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:06:03.801+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:06:03.797+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:06:03.803+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:06:03.854+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:06:34.799+0000] {processor.py:161} INFO - Started process (PID=5586) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:06:34.802+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:06:34.805+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:06:34.804+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:06:34.829+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:06:34.825+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:06:34.830+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:06:34.883+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:07:05.803+0000] {processor.py:161} INFO - Started process (PID=5617) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:07:05.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:07:05.809+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:07:05.808+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:07:05.833+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:07:05.828+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:07:05.834+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:07:05.886+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:07:36.942+0000] {processor.py:161} INFO - Started process (PID=5649) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:07:36.946+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:07:36.948+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:07:36.947+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:07:36.973+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:07:36.968+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:07:36.974+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:07:37.025+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T20:08:07.954+0000] {processor.py:161} INFO - Started process (PID=5680) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:08:07.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:08:07.961+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:08:07.960+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:08:07.991+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:08:07.984+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:08:07.992+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:08:08.053+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.108 seconds
[2024-04-30T20:08:38.971+0000] {processor.py:161} INFO - Started process (PID=5711) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:08:38.974+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:08:38.978+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:08:38.977+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:08:39.002+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:08:38.997+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:08:39.003+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:08:39.057+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:09:09.203+0000] {processor.py:161} INFO - Started process (PID=5742) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:09:09.208+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:09:09.211+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:09:09.210+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:09:09.236+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:09:09.231+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:09:09.237+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:09:09.291+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T20:09:40.307+0000] {processor.py:161} INFO - Started process (PID=5773) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:09:40.316+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:09:40.319+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:09:40.318+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:09:40.343+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:09:40.339+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:09:40.345+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:09:40.397+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.099 seconds
[2024-04-30T20:10:11.355+0000] {processor.py:161} INFO - Started process (PID=5810) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:10:11.358+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:10:11.361+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:10:11.360+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:10:11.385+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:10:11.381+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:10:11.386+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:10:11.437+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T20:10:41.599+0000] {processor.py:161} INFO - Started process (PID=5842) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:10:41.603+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:10:41.605+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:10:41.604+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:10:41.630+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:10:41.625+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:10:41.632+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:10:42.112+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.522 seconds
[2024-04-30T20:11:12.666+0000] {processor.py:161} INFO - Started process (PID=5873) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:11:12.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:11:12.672+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:11:12.671+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:11:12.697+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:11:12.692+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:11:12.698+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:11:12.751+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:11:43.689+0000] {processor.py:161} INFO - Started process (PID=5904) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:11:43.692+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:11:43.695+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:11:43.694+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:11:43.719+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:11:43.714+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:11:43.720+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:11:43.771+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.091 seconds
[2024-04-30T20:12:13.916+0000] {processor.py:161} INFO - Started process (PID=5935) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:12:13.919+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:12:13.922+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:12:13.921+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:12:13.946+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:12:13.942+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:12:13.947+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:12:13.997+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T20:12:44.462+0000] {processor.py:161} INFO - Started process (PID=5966) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:12:44.465+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:12:44.468+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:12:44.467+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:12:44.493+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:12:44.488+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:12:44.494+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:12:44.548+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T20:13:14.825+0000] {processor.py:161} INFO - Started process (PID=5997) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:13:14.829+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:13:14.832+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:13:14.831+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:13:14.857+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:13:14.852+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:13:14.858+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:13:14.914+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.100 seconds
[2024-04-30T20:13:45.229+0000] {processor.py:161} INFO - Started process (PID=6028) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:13:45.232+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:13:45.235+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:13:45.234+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:13:45.261+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:13:45.256+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:13:45.262+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:13:45.316+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T20:14:15.637+0000] {processor.py:161} INFO - Started process (PID=6059) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:14:15.641+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:14:15.644+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:14:15.643+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:14:15.670+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:14:15.665+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:14:15.672+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:14:15.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.123 seconds
[2024-04-30T20:14:46.029+0000] {processor.py:161} INFO - Started process (PID=6089) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:14:46.032+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:14:46.035+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:14:46.034+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:14:46.059+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:14:46.055+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:14:46.061+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:14:46.114+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:15:16.234+0000] {processor.py:161} INFO - Started process (PID=6122) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:15:16.238+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:15:16.241+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:15:16.240+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:15:16.264+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:15:16.260+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:15:16.266+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:15:16.317+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T20:15:47.364+0000] {processor.py:161} INFO - Started process (PID=6153) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:15:47.368+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:15:47.371+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:15:47.370+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:15:47.395+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:15:47.390+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:15:47.396+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:15:47.448+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:16:18.116+0000] {processor.py:161} INFO - Started process (PID=6184) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:16:18.119+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:16:18.122+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:16:18.121+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:16:18.146+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:16:18.141+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:16:18.148+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:16:18.201+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T20:16:48.455+0000] {processor.py:161} INFO - Started process (PID=6215) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:16:48.461+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:16:48.465+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:16:48.464+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:16:48.504+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:16:48.497+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:16:48.506+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:16:48.570+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.132 seconds
[2024-04-30T20:17:18.678+0000] {processor.py:161} INFO - Started process (PID=6246) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:17:18.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:17:18.685+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:17:18.684+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:17:18.709+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:17:18.705+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:17:18.711+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:17:18.765+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T20:17:49.035+0000] {processor.py:161} INFO - Started process (PID=6277) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:17:49.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:17:49.041+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:17:49.040+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:17:49.065+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:17:49.060+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:17:49.066+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:17:49.119+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.106 seconds
[2024-04-30T20:18:19.204+0000] {processor.py:161} INFO - Started process (PID=6307) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:18:19.207+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:18:19.210+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:18:19.209+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:18:19.234+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:18:19.230+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:18:19.236+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:18:19.289+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:18:49.486+0000] {processor.py:161} INFO - Started process (PID=6338) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:18:49.490+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:18:49.494+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:18:49.492+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:18:49.521+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:18:49.515+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:18:49.522+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:18:49.576+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.101 seconds
[2024-04-30T20:19:19.878+0000] {processor.py:161} INFO - Started process (PID=6369) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:19:19.882+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:19:19.885+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:19:19.884+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:19:19.910+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:19:19.905+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:19:19.911+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:19:19.965+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T20:19:50.208+0000] {processor.py:161} INFO - Started process (PID=6400) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:19:50.211+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:19:50.214+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:19:50.213+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:19:50.239+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:19:50.234+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:19:50.240+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:19:50.294+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T20:20:20.916+0000] {processor.py:161} INFO - Started process (PID=6431) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:20:20.920+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:20:20.922+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:20:20.921+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:20:20.947+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:20:20.942+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:20:20.948+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:20:21.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:20:51.697+0000] {processor.py:161} INFO - Started process (PID=6462) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:20:51.700+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:20:51.703+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:20:51.702+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:20:51.728+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:20:51.723+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:20:51.729+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:20:51.785+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.099 seconds
[2024-04-30T20:21:22.384+0000] {processor.py:161} INFO - Started process (PID=6493) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:21:22.388+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:21:22.390+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:21:22.389+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:21:22.415+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:21:22.410+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:21:22.416+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:21:22.469+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:21:53.381+0000] {processor.py:161} INFO - Started process (PID=6524) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:21:53.384+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:21:53.387+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:21:53.386+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:21:53.412+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:21:53.407+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:21:53.413+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:21:53.465+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:22:23.688+0000] {processor.py:161} INFO - Started process (PID=6556) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:22:23.692+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:22:23.694+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:22:23.693+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:22:23.719+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:22:23.714+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:22:23.720+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:22:23.774+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.099 seconds
[2024-04-30T20:22:54.077+0000] {processor.py:161} INFO - Started process (PID=6587) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:22:54.082+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:22:54.087+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:22:54.086+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:22:54.115+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:22:54.109+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:22:54.117+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:22:54.188+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.123 seconds
[2024-04-30T20:23:24.493+0000] {processor.py:161} INFO - Started process (PID=6618) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:23:24.496+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:23:24.499+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:23:24.498+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:23:24.524+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:23:24.519+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:23:24.525+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:23:24.577+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T20:23:55.481+0000] {processor.py:161} INFO - Started process (PID=6649) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:23:55.485+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:23:55.488+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:23:55.487+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:23:55.518+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:23:55.513+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:23:55.520+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:23:55.582+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.112 seconds
[2024-04-30T20:24:26.522+0000] {processor.py:161} INFO - Started process (PID=6680) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:24:26.525+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:24:26.528+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:24:26.527+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:24:26.553+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:24:26.548+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:24:26.555+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:24:26.607+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T20:24:56.679+0000] {processor.py:161} INFO - Started process (PID=6711) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:24:56.683+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:24:56.687+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:24:56.685+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:24:56.712+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:24:56.707+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:24:56.713+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:24:56.769+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.101 seconds
[2024-04-30T20:25:27.669+0000] {processor.py:161} INFO - Started process (PID=6742) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:25:27.672+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:25:27.674+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:25:27.673+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:25:27.699+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:25:27.694+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:25:27.700+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:25:27.754+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T20:25:58.426+0000] {processor.py:161} INFO - Started process (PID=6773) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:25:58.430+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:25:58.433+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:25:58.432+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:25:58.458+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:25:58.453+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:25:58.459+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:25:58.514+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T20:26:29.478+0000] {processor.py:161} INFO - Started process (PID=6804) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:26:29.481+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:26:29.484+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:26:29.483+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:26:29.508+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:26:29.503+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:26:29.509+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:26:29.560+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:27:00.473+0000] {processor.py:161} INFO - Started process (PID=6835) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:27:00.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:27:00.478+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:27:00.477+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:27:00.503+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:27:00.498+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:27:00.504+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:27:00.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:27:31.473+0000] {processor.py:161} INFO - Started process (PID=6866) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:27:31.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:27:31.479+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:27:31.478+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:27:31.503+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:27:31.498+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:27:31.504+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:27:31.555+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:28:02.506+0000] {processor.py:161} INFO - Started process (PID=6897) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:28:02.509+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:28:02.512+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:28:02.511+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:28:02.536+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:28:02.531+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:28:02.537+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:28:02.589+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:28:33.501+0000] {processor.py:161} INFO - Started process (PID=6929) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:28:33.505+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:28:33.508+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:28:33.507+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:28:33.532+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:28:33.527+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:28:33.533+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:28:33.586+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T20:29:04.543+0000] {processor.py:161} INFO - Started process (PID=6960) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:29:04.546+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:29:04.549+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:29:04.548+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:29:04.573+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:29:04.569+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:29:04.575+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:29:04.626+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:29:35.613+0000] {processor.py:161} INFO - Started process (PID=6991) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:29:35.617+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:29:35.620+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:29:35.619+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:29:35.646+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:29:35.641+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:29:35.648+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:29:35.700+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T20:30:06.622+0000] {processor.py:161} INFO - Started process (PID=7022) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:30:06.625+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:30:06.627+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:30:06.626+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:30:06.652+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:30:06.647+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:30:06.653+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:30:06.704+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T20:30:37.605+0000] {processor.py:161} INFO - Started process (PID=7053) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:30:37.608+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:30:37.611+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:30:37.610+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:30:37.635+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:30:37.631+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:30:37.637+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:30:37.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:31:08.681+0000] {processor.py:161} INFO - Started process (PID=7084) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:31:08.688+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:31:08.692+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:31:08.691+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:31:08.716+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:31:08.711+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:31:08.717+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:31:08.768+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.108 seconds
[2024-04-30T20:31:39.651+0000] {processor.py:161} INFO - Started process (PID=7115) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:31:39.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:31:39.658+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:31:39.657+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:31:39.683+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:31:39.678+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:31:39.684+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:31:39.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:32:10.696+0000] {processor.py:161} INFO - Started process (PID=7147) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:32:10.699+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:32:10.702+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:32:10.701+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:32:10.726+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:32:10.722+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:32:10.727+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:32:10.779+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:32:41.741+0000] {processor.py:161} INFO - Started process (PID=7178) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:32:41.745+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:32:41.748+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:32:41.747+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:32:41.773+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:32:41.768+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:32:41.774+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:32:41.827+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:33:12.474+0000] {processor.py:161} INFO - Started process (PID=7209) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:33:12.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:33:12.479+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:33:12.478+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:33:12.503+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:33:12.499+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:33:12.505+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:33:12.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:33:43.529+0000] {processor.py:161} INFO - Started process (PID=7240) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:33:43.532+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:33:43.535+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:33:43.534+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:33:43.559+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:33:43.555+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:33:43.561+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:33:43.611+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T20:34:14.578+0000] {processor.py:161} INFO - Started process (PID=7271) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:34:14.581+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:34:14.584+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:34:14.583+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:34:14.608+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:34:14.604+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:34:14.610+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:34:14.661+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.103 seconds
[2024-04-30T20:34:45.631+0000] {processor.py:161} INFO - Started process (PID=7302) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:34:45.634+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:34:45.637+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:34:45.636+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:34:45.661+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:34:45.657+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:34:45.662+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:34:45.713+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:35:16.687+0000] {processor.py:161} INFO - Started process (PID=7333) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:35:16.690+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:35:16.693+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:35:16.692+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:35:16.717+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:35:16.712+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:35:16.718+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:35:16.769+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T20:35:47.661+0000] {processor.py:161} INFO - Started process (PID=7364) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:35:47.665+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:35:47.669+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:35:47.668+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:35:47.696+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:35:47.691+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:35:47.697+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:35:47.749+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T20:36:18.410+0000] {processor.py:161} INFO - Started process (PID=7395) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:36:18.413+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:36:18.416+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:36:18.415+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:36:18.440+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:36:18.436+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:36:18.442+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:36:18.494+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:36:49.420+0000] {processor.py:161} INFO - Started process (PID=7433) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:36:49.427+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:36:49.430+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:36:49.429+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:36:49.458+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:36:49.452+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:36:49.459+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:36:49.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.107 seconds
[2024-04-30T20:37:20.392+0000] {processor.py:161} INFO - Started process (PID=7464) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:37:20.395+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:37:20.398+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:37:20.397+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:37:20.422+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:37:20.417+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:37:20.423+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:37:20.497+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T20:37:51.051+0000] {processor.py:161} INFO - Started process (PID=7495) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:37:51.054+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:37:51.056+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:37:51.055+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:37:51.081+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:37:51.076+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:37:51.082+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:37:51.136+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T20:38:22.091+0000] {processor.py:161} INFO - Started process (PID=7526) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:38:22.094+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:38:22.097+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:38:22.096+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:38:22.121+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:38:22.117+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:38:22.122+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:38:22.175+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:38:53.143+0000] {processor.py:161} INFO - Started process (PID=7557) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:38:53.147+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:38:53.150+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:38:53.149+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:38:53.174+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:38:53.169+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:38:53.175+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:38:53.227+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:39:23.928+0000] {processor.py:161} INFO - Started process (PID=7588) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:39:23.931+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:39:23.934+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:39:23.933+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:39:23.961+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:39:23.954+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:39:23.963+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:39:24.026+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.108 seconds
[2024-04-30T20:39:54.970+0000] {processor.py:161} INFO - Started process (PID=7619) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:39:54.974+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:39:54.977+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:39:54.976+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:39:55.002+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:39:54.997+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:39:55.003+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:39:55.058+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T20:40:26.019+0000] {processor.py:161} INFO - Started process (PID=7650) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:40:26.023+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:40:26.026+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:40:26.025+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:40:26.050+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:40:26.045+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:40:26.052+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:40:26.103+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:40:57.036+0000] {processor.py:161} INFO - Started process (PID=7681) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:40:57.039+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:40:57.042+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:40:57.041+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:40:57.067+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:40:57.062+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:40:57.068+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:40:57.120+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:41:27.784+0000] {processor.py:161} INFO - Started process (PID=7712) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:41:27.788+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:41:27.790+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:41:27.789+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:41:27.814+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:41:27.810+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:41:27.816+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:41:27.870+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T20:41:58.835+0000] {processor.py:161} INFO - Started process (PID=7743) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:41:58.839+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:41:58.842+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:41:58.841+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:41:58.868+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:41:58.863+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:41:58.869+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:41:58.921+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T20:42:29.002+0000] {processor.py:161} INFO - Started process (PID=7774) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:42:29.005+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:42:29.008+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:42:29.007+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:42:29.033+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:42:29.028+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:42:29.034+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:42:29.087+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T20:43:00.046+0000] {processor.py:161} INFO - Started process (PID=7805) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:43:00.050+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:43:00.053+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:43:00.052+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:43:00.078+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:43:00.073+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:43:00.079+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:43:00.132+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:43:31.031+0000] {processor.py:161} INFO - Started process (PID=7836) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:43:31.034+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:43:31.037+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:43:31.036+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:43:31.061+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:43:31.056+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:43:31.062+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:43:31.114+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:44:02.021+0000] {processor.py:161} INFO - Started process (PID=7867) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:44:02.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:44:02.028+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:44:02.027+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:44:02.055+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:44:02.050+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:44:02.056+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:44:02.108+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T20:44:33.065+0000] {processor.py:161} INFO - Started process (PID=7898) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:44:33.069+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:44:33.072+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:44:33.071+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:44:33.097+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:44:33.092+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:44:33.099+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:44:33.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T20:45:04.127+0000] {processor.py:161} INFO - Started process (PID=7929) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:45:04.130+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:45:04.133+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:45:04.132+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:45:04.158+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:45:04.153+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:45:04.160+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:45:04.212+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:45:35.126+0000] {processor.py:161} INFO - Started process (PID=7960) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:45:35.130+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:45:35.133+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:45:35.132+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:45:35.157+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:45:35.152+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:45:35.158+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:45:35.210+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:46:06.092+0000] {processor.py:161} INFO - Started process (PID=7992) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:46:06.096+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:46:06.100+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:46:06.098+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:46:06.125+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:46:06.120+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:46:06.126+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:46:06.178+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T20:46:37.189+0000] {processor.py:161} INFO - Started process (PID=8023) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:46:37.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:46:37.196+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:46:37.195+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:46:37.222+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:46:37.217+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:46:37.223+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:46:37.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:47:08.201+0000] {processor.py:161} INFO - Started process (PID=8054) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:47:08.204+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:47:08.207+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:47:08.206+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:47:08.238+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:47:08.233+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:47:08.239+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:47:08.291+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.102 seconds
[2024-04-30T20:47:39.173+0000] {processor.py:161} INFO - Started process (PID=8085) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:47:39.177+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:47:39.180+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:47:39.179+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:47:39.205+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:47:39.200+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:47:39.206+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:47:39.260+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T20:48:10.290+0000] {processor.py:161} INFO - Started process (PID=8116) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:48:10.293+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:48:10.296+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:48:10.295+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:48:10.320+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:48:10.315+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:48:10.321+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:48:10.373+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:48:41.298+0000] {processor.py:161} INFO - Started process (PID=8147) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:48:41.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:48:41.304+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:48:41.303+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:48:41.329+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:48:41.324+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:48:41.330+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:48:41.392+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.104 seconds
[2024-04-30T20:49:12.364+0000] {processor.py:161} INFO - Started process (PID=8179) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:49:12.368+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:49:12.371+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:49:12.370+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:49:12.395+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:49:12.391+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:49:12.397+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:49:12.450+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T20:49:43.406+0000] {processor.py:161} INFO - Started process (PID=8210) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:49:43.410+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:49:43.412+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:49:43.411+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:49:43.437+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:49:43.432+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:49:43.438+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:49:43.490+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:50:14.389+0000] {processor.py:161} INFO - Started process (PID=8241) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:50:14.393+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:50:14.396+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:50:14.395+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:50:14.420+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:50:14.415+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:50:14.422+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:50:14.474+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T20:50:45.417+0000] {processor.py:161} INFO - Started process (PID=8272) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:50:45.420+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:50:45.424+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:50:45.423+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:50:45.449+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:50:45.444+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:50:45.451+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:50:45.504+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T20:51:16.459+0000] {processor.py:161} INFO - Started process (PID=8303) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:51:16.463+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:51:16.466+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:51:16.465+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:51:16.491+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:51:16.486+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:51:16.492+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:51:16.572+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.122 seconds
[2024-04-30T20:51:47.508+0000] {processor.py:161} INFO - Started process (PID=8334) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:51:47.511+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:51:47.514+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:51:47.513+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:51:47.539+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:51:47.534+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:51:47.540+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:51:47.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:52:18.546+0000] {processor.py:161} INFO - Started process (PID=8365) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:52:18.550+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:52:18.553+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:52:18.552+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:52:18.580+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:52:18.576+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:52:18.582+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:52:18.634+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T20:52:49.643+0000] {processor.py:161} INFO - Started process (PID=8396) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:52:49.647+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:52:49.650+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:52:49.649+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:52:49.675+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:52:49.670+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:52:49.676+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:52:49.730+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:53:20.712+0000] {processor.py:161} INFO - Started process (PID=8427) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:53:20.720+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:53:20.727+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:53:20.723+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:53:20.766+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:53:20.760+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:53:20.769+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:53:20.821+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.124 seconds
[2024-04-30T20:53:51.496+0000] {processor.py:161} INFO - Started process (PID=8458) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:53:51.499+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:53:51.501+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:53:51.500+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:53:51.525+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:53:51.521+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:53:51.527+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:53:51.578+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T20:54:22.468+0000] {processor.py:161} INFO - Started process (PID=8490) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:54:22.472+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:54:22.476+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:54:22.475+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:54:22.504+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:54:22.498+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:54:22.505+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:54:22.564+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.109 seconds
[2024-04-30T20:54:53.506+0000] {processor.py:161} INFO - Started process (PID=8521) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:54:53.509+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:54:53.512+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:54:53.511+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:54:53.536+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:54:53.531+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:54:53.537+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:54:53.588+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T20:55:24.537+0000] {processor.py:161} INFO - Started process (PID=8552) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:55:24.541+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:55:24.544+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:55:24.543+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:55:24.569+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:55:24.564+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:55:24.570+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:55:24.626+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T20:55:55.540+0000] {processor.py:161} INFO - Started process (PID=8589) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:55:55.543+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:55:55.546+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:55:55.545+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:55:55.571+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:55:55.566+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:55:55.573+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:55:55.625+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T20:56:26.572+0000] {processor.py:161} INFO - Started process (PID=8620) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:56:26.576+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:56:26.578+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:56:26.577+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:56:26.603+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:56:26.598+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:56:26.604+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:56:26.655+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T20:56:57.568+0000] {processor.py:161} INFO - Started process (PID=8651) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:56:57.571+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:56:57.574+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:56:57.573+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:56:57.604+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:56:57.597+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:56:57.606+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:56:57.667+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.109 seconds
[2024-04-30T20:57:28.709+0000] {processor.py:161} INFO - Started process (PID=8683) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:57:28.712+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:57:28.715+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:57:28.714+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:57:28.740+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:57:28.735+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:57:28.741+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:57:28.794+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T20:57:59.702+0000] {processor.py:161} INFO - Started process (PID=8714) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:57:59.706+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:57:59.709+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:57:59.708+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:57:59.733+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:57:59.729+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:57:59.735+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:57:59.788+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T20:58:30.737+0000] {processor.py:161} INFO - Started process (PID=8745) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:58:30.740+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:58:30.743+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:58:30.741+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:58:30.767+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:58:30.762+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:58:30.768+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:58:30.820+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T20:59:01.719+0000] {processor.py:161} INFO - Started process (PID=8776) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:59:01.723+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:59:01.727+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:59:01.726+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:59:01.763+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:59:01.757+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:59:01.764+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:59:01.825+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.120 seconds
[2024-04-30T20:59:31.902+0000] {processor.py:161} INFO - Started process (PID=8808) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T20:59:31.905+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T20:59:31.908+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:59:31.907+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:59:31.939+0000] {logging_mixin.py:188} INFO - [2024-04-30T20:59:31.931+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T20:59:31.944+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T20:59:31.998+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.108 seconds
[2024-04-30T21:00:02.872+0000] {processor.py:161} INFO - Started process (PID=8839) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:00:02.876+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:00:02.879+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:00:02.878+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:00:02.904+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:00:02.899+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:00:02.905+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:00:02.957+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:00:33.866+0000] {processor.py:161} INFO - Started process (PID=8871) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:00:33.870+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:00:33.873+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:00:33.872+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:00:33.900+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:00:33.895+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:00:33.901+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:00:33.956+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.100 seconds
[2024-04-30T21:01:04.914+0000] {processor.py:161} INFO - Started process (PID=8902) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:01:04.917+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:01:04.920+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:01:04.919+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:01:04.945+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:01:04.940+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:01:04.946+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:01:04.999+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:01:35.924+0000] {processor.py:161} INFO - Started process (PID=8933) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:01:35.927+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:01:35.931+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:01:35.929+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:01:35.958+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:01:35.952+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:01:35.959+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:01:36.012+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.099 seconds
[2024-04-30T21:02:06.955+0000] {processor.py:161} INFO - Started process (PID=8964) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:02:06.959+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:02:06.962+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:02:06.961+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:02:06.987+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:02:06.982+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:02:06.989+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:02:07.040+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:02:38.013+0000] {processor.py:161} INFO - Started process (PID=8995) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:02:38.017+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:02:38.020+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:02:38.019+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:02:38.046+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:02:38.041+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:02:38.047+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:02:38.102+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.099 seconds
[2024-04-30T21:03:09.099+0000] {processor.py:161} INFO - Started process (PID=9026) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:03:09.103+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:03:09.107+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:03:09.105+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:03:09.142+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:03:09.135+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:03:09.143+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:03:09.197+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T21:03:39.861+0000] {processor.py:161} INFO - Started process (PID=9057) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:03:39.864+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:03:39.868+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:03:39.867+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:03:39.893+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:03:39.888+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:03:39.894+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:03:39.948+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T21:04:10.941+0000] {processor.py:161} INFO - Started process (PID=9089) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:04:10.947+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:04:10.952+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:04:10.951+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:04:10.984+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:04:10.979+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:04:10.985+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:04:11.037+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T21:04:42.011+0000] {processor.py:161} INFO - Started process (PID=9120) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:04:42.014+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:04:42.017+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:04:42.016+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:04:42.041+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:04:42.037+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:04:42.043+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:04:42.094+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:05:13.027+0000] {processor.py:161} INFO - Started process (PID=9151) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:05:13.030+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:05:13.034+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:05:13.031+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:05:13.086+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:05:13.077+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:05:13.089+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:05:13.190+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.174 seconds
[2024-04-30T21:05:44.070+0000] {processor.py:161} INFO - Started process (PID=9182) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:05:44.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:05:44.076+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:05:44.075+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:05:44.100+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:05:44.095+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:05:44.101+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:05:44.153+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:06:15.103+0000] {processor.py:161} INFO - Started process (PID=9213) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:06:15.106+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:06:15.109+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:06:15.108+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:06:15.133+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:06:15.128+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:06:15.134+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:06:15.186+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:06:46.160+0000] {processor.py:161} INFO - Started process (PID=9244) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:06:46.163+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:06:46.166+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:06:46.165+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:06:46.190+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:06:46.186+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:06:46.191+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:06:46.243+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:07:17.120+0000] {processor.py:161} INFO - Started process (PID=9275) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:07:17.124+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:07:17.127+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:07:17.126+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:07:17.180+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:07:17.164+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:07:17.183+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:07:17.274+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.165 seconds
[2024-04-30T21:07:48.129+0000] {processor.py:161} INFO - Started process (PID=9307) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:07:48.132+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:07:48.135+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:07:48.134+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:07:48.162+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:07:48.157+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:07:48.164+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:07:48.215+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T21:08:19.122+0000] {processor.py:161} INFO - Started process (PID=9338) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:08:19.125+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:08:19.128+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:08:19.127+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:08:19.152+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:08:19.147+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:08:19.153+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:08:19.204+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:08:50.104+0000] {processor.py:161} INFO - Started process (PID=9370) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:08:50.107+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:08:50.110+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:08:50.109+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:08:50.134+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:08:50.129+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:08:50.136+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:08:50.206+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.111 seconds
[2024-04-30T21:09:21.165+0000] {processor.py:161} INFO - Started process (PID=9401) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:09:21.168+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:09:21.171+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:09:21.170+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:09:21.195+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:09:21.190+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:09:21.196+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:09:21.247+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:09:52.290+0000] {processor.py:161} INFO - Started process (PID=9433) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:09:52.294+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:09:52.297+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:09:52.296+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:09:52.323+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:09:52.318+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:09:52.324+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:09:52.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.103 seconds
[2024-04-30T21:10:23.298+0000] {processor.py:161} INFO - Started process (PID=9464) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:10:23.300+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:10:23.303+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:10:23.302+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:10:23.327+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:10:23.323+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:10:23.328+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:10:23.379+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.091 seconds
[2024-04-30T21:10:54.028+0000] {processor.py:161} INFO - Started process (PID=9496) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:10:54.032+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:10:54.035+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:10:54.034+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:10:54.060+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:10:54.055+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:10:54.061+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:10:54.113+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:11:25.059+0000] {processor.py:161} INFO - Started process (PID=9528) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:11:25.062+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:11:25.065+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:11:25.064+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:11:25.089+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:11:25.085+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:11:25.091+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:11:25.151+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.102 seconds
[2024-04-30T21:11:56.131+0000] {processor.py:161} INFO - Started process (PID=9559) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:11:56.134+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:11:56.137+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:11:56.136+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:11:56.161+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:11:56.157+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:11:56.163+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:11:56.213+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:12:27.140+0000] {processor.py:161} INFO - Started process (PID=9590) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:12:27.143+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:12:27.146+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:12:27.145+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:12:27.170+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:12:27.166+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:12:27.172+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:12:27.224+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:12:58.233+0000] {processor.py:161} INFO - Started process (PID=9621) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:12:58.236+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:12:58.239+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:12:58.238+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:12:58.264+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:12:58.259+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:12:58.265+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:12:58.317+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T21:13:29.265+0000] {processor.py:161} INFO - Started process (PID=9652) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:13:29.268+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:13:29.271+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:13:29.270+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:13:29.295+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:13:29.290+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:13:29.296+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:13:29.347+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:14:00.296+0000] {processor.py:161} INFO - Started process (PID=9683) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:14:00.299+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:14:00.302+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:14:00.301+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:14:00.326+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:14:00.322+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:14:00.328+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:14:00.379+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:14:30.467+0000] {processor.py:161} INFO - Started process (PID=9714) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:14:30.470+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:14:30.473+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:14:30.472+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:14:30.497+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:14:30.493+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:14:30.499+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:14:30.554+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T21:15:01.493+0000] {processor.py:161} INFO - Started process (PID=9746) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:15:01.505+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:15:01.509+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:15:01.507+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:15:01.536+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:15:01.531+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:15:01.538+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:15:01.601+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.119 seconds
[2024-04-30T21:15:32.569+0000] {processor.py:161} INFO - Started process (PID=9783) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:15:32.572+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:15:32.576+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:15:32.574+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:15:32.600+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:15:32.595+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:15:32.602+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:15:32.653+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:16:03.624+0000] {processor.py:161} INFO - Started process (PID=9814) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:16:03.628+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:16:03.631+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:16:03.630+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:16:03.656+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:16:03.651+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:16:03.657+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:16:03.709+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:16:34.444+0000] {processor.py:161} INFO - Started process (PID=9845) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:16:34.447+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:16:34.450+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:16:34.449+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:16:34.474+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:16:34.469+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:16:34.475+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:16:34.527+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:17:05.486+0000] {processor.py:161} INFO - Started process (PID=9876) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:17:05.491+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:17:05.495+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:17:05.493+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:17:05.533+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:17:05.526+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:17:05.534+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:17:05.590+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.124 seconds
[2024-04-30T21:17:36.547+0000] {processor.py:161} INFO - Started process (PID=9907) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:17:36.551+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:17:36.554+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:17:36.553+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:17:36.578+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:17:36.574+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:17:36.579+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:17:36.633+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.108 seconds
[2024-04-30T21:18:07.561+0000] {processor.py:161} INFO - Started process (PID=9939) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:18:07.564+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:18:07.568+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:18:07.566+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:18:07.592+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:18:07.587+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:18:07.594+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:18:07.645+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:18:38.618+0000] {processor.py:161} INFO - Started process (PID=9970) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:18:38.621+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:18:38.625+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:18:38.623+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:18:38.653+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:18:38.646+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:18:38.655+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:18:38.728+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.125 seconds
[2024-04-30T21:19:09.697+0000] {processor.py:161} INFO - Started process (PID=10001) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:19:09.701+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:19:09.704+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:19:09.703+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:19:09.731+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:19:09.725+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:19:09.733+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:19:09.785+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T21:19:40.817+0000] {processor.py:161} INFO - Started process (PID=10032) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:19:40.821+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:19:40.824+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:19:40.823+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:19:40.848+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:19:40.844+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:19:40.849+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:19:40.901+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:20:11.884+0000] {processor.py:161} INFO - Started process (PID=10063) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:20:11.887+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:20:11.889+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:20:11.888+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:20:11.913+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:20:11.909+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:20:11.915+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:20:11.965+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:20:42.872+0000] {processor.py:161} INFO - Started process (PID=10094) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:20:42.875+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:20:42.879+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:20:42.878+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:20:42.903+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:20:42.899+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:20:42.905+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:20:42.957+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:21:13.928+0000] {processor.py:161} INFO - Started process (PID=10125) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:21:13.932+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:21:13.935+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:21:13.934+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:21:13.959+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:21:13.954+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:21:13.961+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:21:14.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:21:44.936+0000] {processor.py:161} INFO - Started process (PID=10156) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:21:44.940+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:21:44.943+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:21:44.942+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:21:44.971+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:21:44.965+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:21:44.972+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:21:45.024+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.099 seconds
[2024-04-30T21:22:15.899+0000] {processor.py:161} INFO - Started process (PID=10187) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:22:15.907+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:22:15.911+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:22:15.909+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:22:15.938+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:22:15.932+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:22:15.939+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:22:15.995+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.107 seconds
[2024-04-30T21:22:46.939+0000] {processor.py:161} INFO - Started process (PID=10218) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:22:46.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:22:46.945+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:22:46.944+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:22:46.969+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:22:46.965+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:22:46.971+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:22:47.021+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:23:18.023+0000] {processor.py:161} INFO - Started process (PID=10249) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:23:18.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:23:18.028+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:23:18.027+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:23:18.052+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:23:18.048+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:23:18.053+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:23:18.105+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:23:49.002+0000] {processor.py:161} INFO - Started process (PID=10280) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:23:49.006+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:23:49.009+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:23:49.008+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:23:49.039+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:23:49.034+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:23:49.041+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:23:49.107+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T21:24:20.069+0000] {processor.py:161} INFO - Started process (PID=10311) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:24:20.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:24:20.076+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:24:20.075+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:24:20.104+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:24:20.099+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:24:20.106+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:24:20.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.110 seconds
[2024-04-30T21:24:51.088+0000] {processor.py:161} INFO - Started process (PID=10342) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:24:51.091+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:24:51.094+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:24:51.093+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:24:51.118+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:24:51.114+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:24:51.120+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:24:51.171+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T21:25:22.140+0000] {processor.py:161} INFO - Started process (PID=10373) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:25:22.143+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:25:22.146+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:25:22.145+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:25:22.170+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:25:22.166+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:25:22.172+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:25:22.223+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:25:53.164+0000] {processor.py:161} INFO - Started process (PID=10404) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:25:53.166+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:25:53.169+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:25:53.168+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:25:53.193+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:25:53.189+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:25:53.194+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:25:53.245+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.091 seconds
[2024-04-30T21:26:24.195+0000] {processor.py:161} INFO - Started process (PID=10435) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:26:24.199+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:26:24.202+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:26:24.201+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:26:24.227+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:26:24.222+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:26:24.228+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:26:24.279+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T21:26:55.198+0000] {processor.py:161} INFO - Started process (PID=10466) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:26:55.201+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:26:55.204+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:26:55.203+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:26:55.230+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:26:55.225+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:26:55.231+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:26:55.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:27:26.247+0000] {processor.py:161} INFO - Started process (PID=10499) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:27:26.251+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:27:26.254+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:27:26.253+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:27:26.278+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:27:26.273+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:27:26.279+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:27:26.333+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:27:57.232+0000] {processor.py:161} INFO - Started process (PID=10530) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:27:57.235+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:27:57.238+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:27:57.237+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:27:57.262+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:27:57.257+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:27:57.263+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:27:57.316+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:28:28.244+0000] {processor.py:161} INFO - Started process (PID=10561) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:28:28.247+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:28:28.250+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:28:28.249+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:28:28.274+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:28:28.269+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:28:28.275+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:28:28.328+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:28:59.239+0000] {processor.py:161} INFO - Started process (PID=10592) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:28:59.242+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:28:59.245+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:28:59.244+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:28:59.269+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:28:59.265+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:28:59.271+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:28:59.323+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:29:30.313+0000] {processor.py:161} INFO - Started process (PID=10623) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:29:30.317+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:29:30.319+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:29:30.318+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:29:30.344+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:29:30.339+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:29:30.345+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:29:30.397+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:30:01.356+0000] {processor.py:161} INFO - Started process (PID=10655) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:30:01.359+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:30:01.362+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:30:01.361+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:30:01.390+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:30:01.385+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:30:01.391+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:30:01.449+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.104 seconds
[2024-04-30T21:30:32.377+0000] {processor.py:161} INFO - Started process (PID=10687) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:30:32.381+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:30:32.384+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:30:32.382+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:30:32.408+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:30:32.403+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:30:32.409+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:30:32.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:31:03.495+0000] {processor.py:161} INFO - Started process (PID=10718) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:31:03.498+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:31:03.500+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:31:03.499+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:31:03.525+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:31:03.520+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:31:03.526+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:31:03.577+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:31:34.475+0000] {processor.py:161} INFO - Started process (PID=10749) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:31:34.479+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:31:34.482+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:31:34.481+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:31:34.506+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:31:34.502+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:31:34.508+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:31:34.559+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:32:05.454+0000] {processor.py:161} INFO - Started process (PID=10780) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:32:05.456+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:32:05.459+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:32:05.458+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:32:05.483+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:32:05.479+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:32:05.485+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:32:05.538+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:32:36.443+0000] {processor.py:161} INFO - Started process (PID=10812) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:32:36.447+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:32:36.450+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:32:36.449+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:32:36.474+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:32:36.470+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:32:36.476+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:32:36.531+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.099 seconds
[2024-04-30T21:33:07.438+0000] {processor.py:161} INFO - Started process (PID=10849) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:33:07.440+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:33:07.444+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:33:07.442+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:33:07.489+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:33:07.480+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:33:07.494+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:33:07.548+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.126 seconds
[2024-04-30T21:33:38.512+0000] {processor.py:161} INFO - Started process (PID=10881) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:33:38.516+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:33:38.518+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:33:38.517+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:33:38.543+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:33:38.538+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:33:38.544+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:33:38.597+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:34:09.563+0000] {processor.py:161} INFO - Started process (PID=10912) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:34:09.565+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:34:09.568+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:34:09.567+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:34:09.592+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:34:09.587+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:34:09.593+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:34:09.646+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:34:40.657+0000] {processor.py:161} INFO - Started process (PID=10943) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:34:40.660+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:34:40.663+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:34:40.662+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:34:40.688+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:34:40.683+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:34:40.689+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:34:40.743+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T21:35:11.722+0000] {processor.py:161} INFO - Started process (PID=10974) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:35:11.726+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:35:11.729+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:35:11.728+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:35:11.753+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:35:11.748+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:35:11.754+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:35:11.805+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:35:42.711+0000] {processor.py:161} INFO - Started process (PID=11005) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:35:42.717+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:35:42.724+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:35:42.721+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:35:42.779+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:35:42.768+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:35:42.782+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:35:42.845+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.158 seconds
[2024-04-30T21:36:13.693+0000] {processor.py:161} INFO - Started process (PID=11036) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:36:13.697+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:36:13.699+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:36:13.698+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:36:13.723+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:36:13.719+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:36:13.725+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:36:13.777+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:36:44.685+0000] {processor.py:161} INFO - Started process (PID=11067) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:36:44.688+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:36:44.691+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:36:44.690+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:36:44.715+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:36:44.710+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:36:44.716+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:36:44.767+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.091 seconds
[2024-04-30T21:37:15.674+0000] {processor.py:161} INFO - Started process (PID=11098) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:37:15.677+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:37:15.680+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:37:15.679+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:37:15.704+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:37:15.700+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:37:15.706+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:37:15.772+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.108 seconds
[2024-04-30T21:37:46.714+0000] {processor.py:161} INFO - Started process (PID=11129) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:37:46.716+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:37:46.719+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:37:46.718+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:37:46.743+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:37:46.739+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:37:46.744+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:37:46.795+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.091 seconds
[2024-04-30T21:38:17.786+0000] {processor.py:161} INFO - Started process (PID=11160) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:38:17.790+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:38:17.793+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:38:17.791+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:38:17.817+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:38:17.812+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:38:17.818+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:38:17.870+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:38:48.792+0000] {processor.py:161} INFO - Started process (PID=11191) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:38:48.795+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:38:48.798+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:38:48.797+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:38:48.822+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:38:48.817+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:38:48.823+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:38:48.875+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:39:19.562+0000] {processor.py:161} INFO - Started process (PID=11222) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:39:19.565+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:39:19.568+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:39:19.567+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:39:19.592+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:39:19.587+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:39:19.593+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:39:19.644+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:39:50.368+0000] {processor.py:161} INFO - Started process (PID=11253) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:39:50.372+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:39:50.375+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:39:50.374+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:39:50.399+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:39:50.395+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:39:50.401+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:39:50.451+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:40:21.431+0000] {processor.py:161} INFO - Started process (PID=11284) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:40:21.439+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:40:21.443+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:40:21.441+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:40:21.467+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:40:21.463+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:40:21.469+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:40:21.521+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.100 seconds
[2024-04-30T21:40:52.524+0000] {processor.py:161} INFO - Started process (PID=11315) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:40:52.528+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:40:52.530+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:40:52.529+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:40:52.555+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:40:52.550+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:40:52.556+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:40:52.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:41:23.542+0000] {processor.py:161} INFO - Started process (PID=11346) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:41:23.546+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:41:23.550+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:41:23.549+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:41:23.574+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:41:23.570+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:41:23.576+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:41:23.632+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.099 seconds
[2024-04-30T21:41:54.228+0000] {processor.py:161} INFO - Started process (PID=11377) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:41:54.231+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:41:54.234+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:41:54.233+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:41:54.260+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:41:54.255+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:41:54.262+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:41:54.324+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.106 seconds
[2024-04-30T21:42:25.202+0000] {processor.py:161} INFO - Started process (PID=11408) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:42:25.206+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:42:25.210+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:42:25.209+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:42:25.237+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:42:25.232+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:42:25.239+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:42:25.299+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.107 seconds
[2024-04-30T21:42:56.246+0000] {processor.py:161} INFO - Started process (PID=11439) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:42:56.250+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:42:56.253+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:42:56.252+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:42:56.278+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:42:56.273+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:42:56.279+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:42:56.332+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T21:43:27.257+0000] {processor.py:161} INFO - Started process (PID=11470) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:43:27.259+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:43:27.262+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:43:27.261+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:43:27.287+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:43:27.282+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:43:27.288+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:43:27.339+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:43:58.224+0000] {processor.py:161} INFO - Started process (PID=11501) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:43:58.228+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:43:58.231+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:43:58.230+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:43:58.258+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:43:58.253+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:43:58.260+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:43:58.320+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.106 seconds
[2024-04-30T21:44:29.260+0000] {processor.py:161} INFO - Started process (PID=11532) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:44:29.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:44:29.266+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:44:29.265+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:44:29.291+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:44:29.286+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:44:29.292+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:44:29.344+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:45:00.310+0000] {processor.py:161} INFO - Started process (PID=11563) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:45:00.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:45:00.317+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:45:00.315+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:45:00.341+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:45:00.336+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:45:00.342+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:45:00.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:45:31.313+0000] {processor.py:161} INFO - Started process (PID=11594) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:45:31.317+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:45:31.320+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:45:31.319+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:45:31.348+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:45:31.342+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:45:31.349+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:45:31.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.106 seconds
[2024-04-30T21:46:02.331+0000] {processor.py:161} INFO - Started process (PID=11626) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:46:02.335+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:46:02.339+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:46:02.338+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:46:02.375+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:46:02.368+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:46:02.376+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:46:02.440+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.123 seconds
[2024-04-30T21:46:33.315+0000] {processor.py:161} INFO - Started process (PID=11657) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:46:33.318+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:46:33.321+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:46:33.320+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:46:33.345+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:46:33.340+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:46:33.346+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:46:33.396+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:47:04.461+0000] {processor.py:161} INFO - Started process (PID=11688) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:47:04.464+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:47:04.467+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:47:04.466+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:47:04.498+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:47:04.492+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:47:04.499+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:47:04.555+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.104 seconds
[2024-04-30T21:47:35.453+0000] {processor.py:161} INFO - Started process (PID=11719) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:47:35.456+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:47:35.459+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:47:35.458+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:47:35.483+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:47:35.479+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:47:35.485+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:47:35.537+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:48:06.480+0000] {processor.py:161} INFO - Started process (PID=11750) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:48:06.483+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:48:06.485+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:48:06.484+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:48:06.510+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:48:06.505+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:48:06.511+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:48:06.562+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:48:37.473+0000] {processor.py:161} INFO - Started process (PID=11781) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:48:37.477+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:48:37.480+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:48:37.479+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:48:37.505+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:48:37.500+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:48:37.506+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:48:37.557+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T21:49:08.593+0000] {processor.py:161} INFO - Started process (PID=11813) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:49:08.596+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:49:08.599+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:49:08.598+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:49:08.623+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:49:08.618+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:49:08.624+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:49:08.675+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:49:39.680+0000] {processor.py:161} INFO - Started process (PID=11844) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:49:39.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:49:39.686+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:49:39.685+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:49:39.711+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:49:39.706+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:49:39.712+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:49:39.763+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:50:10.729+0000] {processor.py:161} INFO - Started process (PID=11875) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:50:10.733+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:50:10.737+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:50:10.736+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:50:10.762+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:50:10.757+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:50:10.763+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:50:10.817+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.099 seconds
[2024-04-30T21:50:41.870+0000] {processor.py:161} INFO - Started process (PID=11906) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:50:41.873+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:50:41.876+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:50:41.875+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:50:41.900+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:50:41.895+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:50:41.901+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:50:41.951+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.091 seconds
[2024-04-30T21:51:12.937+0000] {processor.py:161} INFO - Started process (PID=11937) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:51:12.940+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:51:12.943+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:51:12.942+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:51:12.967+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:51:12.962+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:51:12.968+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:51:13.038+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.111 seconds
[2024-04-30T21:51:43.689+0000] {processor.py:161} INFO - Started process (PID=11974) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:51:43.692+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:51:43.695+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:51:43.694+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:51:43.719+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:51:43.714+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:51:43.720+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:51:43.795+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T21:52:14.752+0000] {processor.py:161} INFO - Started process (PID=12005) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:52:14.755+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:52:14.758+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:52:14.757+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:52:14.783+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:52:14.778+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:52:14.784+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:52:14.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T21:52:45.796+0000] {processor.py:161} INFO - Started process (PID=12036) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:52:45.800+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:52:45.803+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:52:45.802+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:52:45.830+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:52:45.825+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:52:45.832+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:52:45.884+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.099 seconds
[2024-04-30T21:53:16.826+0000] {processor.py:161} INFO - Started process (PID=12067) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:53:16.830+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:53:16.833+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:53:16.832+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:53:16.858+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:53:16.853+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:53:16.859+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:53:16.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:53:47.554+0000] {processor.py:161} INFO - Started process (PID=12098) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:53:47.557+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:53:47.560+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:53:47.559+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:53:47.584+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:53:47.579+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:53:47.585+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:53:47.637+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T21:54:18.517+0000] {processor.py:161} INFO - Started process (PID=12129) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:54:18.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:54:18.524+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:54:18.523+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:54:18.548+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:54:18.544+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:54:18.550+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:54:18.602+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T21:54:49.700+0000] {processor.py:161} INFO - Started process (PID=12160) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:54:49.706+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:54:49.712+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:54:49.710+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:54:49.739+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:54:49.734+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:54:49.740+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:54:49.792+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T21:55:20.703+0000] {processor.py:161} INFO - Started process (PID=12191) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:55:20.707+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:55:20.710+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:55:20.709+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:55:20.734+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:55:20.729+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:55:20.735+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:55:20.787+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T21:55:51.490+0000] {processor.py:161} INFO - Started process (PID=12222) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:55:51.501+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:55:51.505+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:55:51.503+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:55:51.532+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:55:51.527+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:55:51.534+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:55:51.600+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.129 seconds
[2024-04-30T21:56:22.520+0000] {processor.py:161} INFO - Started process (PID=12253) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:56:22.522+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:56:22.525+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:56:22.524+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:56:22.549+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:56:22.544+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:56:22.550+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:56:22.602+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:56:52.689+0000] {processor.py:161} INFO - Started process (PID=12285) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:56:52.693+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:56:52.696+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:56:52.694+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:56:52.720+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:56:52.715+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:56:52.721+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:56:52.775+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T21:57:23.714+0000] {processor.py:161} INFO - Started process (PID=12316) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:57:23.716+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:57:23.718+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:57:23.717+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:57:23.742+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:57:23.738+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:57:23.744+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:57:23.797+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T21:57:54.778+0000] {processor.py:161} INFO - Started process (PID=12347) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:57:54.781+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:57:54.784+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:57:54.783+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:57:54.809+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:57:54.804+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:57:54.810+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:57:54.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T21:58:25.860+0000] {processor.py:161} INFO - Started process (PID=12378) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:58:25.864+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:58:25.868+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:58:25.867+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:58:25.906+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:58:25.899+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:58:25.908+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:58:25.962+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T21:58:56.874+0000] {processor.py:161} INFO - Started process (PID=12409) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:58:56.877+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:58:56.880+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:58:56.879+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:58:56.904+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:58:56.900+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:58:56.905+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:58:56.956+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T21:59:27.857+0000] {processor.py:161} INFO - Started process (PID=12440) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:59:27.861+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:59:27.864+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:59:27.863+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:59:27.888+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:59:27.884+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:59:27.890+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:59:27.942+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T21:59:58.975+0000] {processor.py:161} INFO - Started process (PID=12471) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T21:59:58.979+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T21:59:58.982+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:59:58.981+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:59:59.020+0000] {logging_mixin.py:188} INFO - [2024-04-30T21:59:59.013+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T21:59:59.021+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T21:59:59.072+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.108 seconds
[2024-04-30T22:00:29.685+0000] {processor.py:161} INFO - Started process (PID=12502) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:00:29.690+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:00:29.693+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:00:29.692+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:00:29.721+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:00:29.716+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:00:29.722+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:00:29.773+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.105 seconds
[2024-04-30T22:01:00.725+0000] {processor.py:161} INFO - Started process (PID=12534) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:01:00.728+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:01:00.731+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:01:00.730+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:01:00.755+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:01:00.750+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:01:00.756+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:01:00.808+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T22:01:31.445+0000] {processor.py:161} INFO - Started process (PID=12565) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:01:31.449+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:01:31.452+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:01:31.451+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:01:31.477+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:01:31.472+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:01:31.478+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:01:31.530+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T22:02:02.439+0000] {processor.py:161} INFO - Started process (PID=12596) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:02:02.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:02:02.447+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:02:02.445+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:02:02.482+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:02:02.475+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:02:02.484+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:02:02.542+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.118 seconds
[2024-04-30T22:02:33.481+0000] {processor.py:161} INFO - Started process (PID=12627) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:02:33.485+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:02:33.488+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:02:33.486+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:02:33.512+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:02:33.507+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:02:33.513+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:02:33.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.103 seconds
[2024-04-30T22:03:04.502+0000] {processor.py:161} INFO - Started process (PID=12658) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:03:04.506+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:03:04.509+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:03:04.508+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:03:04.534+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:03:04.529+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:03:04.535+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:03:04.589+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T22:03:35.500+0000] {processor.py:161} INFO - Started process (PID=12689) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:03:35.503+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:03:35.506+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:03:35.505+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:03:35.530+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:03:35.526+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:03:35.532+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:03:35.587+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T22:04:06.508+0000] {processor.py:161} INFO - Started process (PID=12720) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:04:06.511+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:04:06.515+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:04:06.513+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:04:06.540+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:04:06.535+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:04:06.541+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:04:06.591+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T22:04:37.670+0000] {processor.py:161} INFO - Started process (PID=12751) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:04:37.674+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:04:37.677+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:04:37.676+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:04:37.702+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:04:37.697+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:04:37.703+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:04:37.754+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T22:05:08.702+0000] {processor.py:161} INFO - Started process (PID=12782) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:05:08.705+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:05:08.707+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:05:08.706+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:05:08.731+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:05:08.727+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:05:08.733+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:05:08.784+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T22:05:39.801+0000] {processor.py:161} INFO - Started process (PID=12814) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:05:39.805+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:05:39.808+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:05:39.807+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:05:39.833+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:05:39.828+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:05:39.834+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:05:39.887+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T22:06:10.831+0000] {processor.py:161} INFO - Started process (PID=12845) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:06:10.835+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:06:10.838+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:06:10.837+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:06:10.861+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:06:10.857+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:06:10.863+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:06:10.917+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T22:06:41.917+0000] {processor.py:161} INFO - Started process (PID=12876) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:06:41.920+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:06:41.923+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:06:41.921+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:06:41.950+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:06:41.945+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:06:41.951+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:06:42.006+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.100 seconds
[2024-04-30T22:07:12.998+0000] {processor.py:161} INFO - Started process (PID=12907) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:07:13.002+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:07:13.004+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:07:13.003+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:07:13.028+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:07:13.024+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:07:13.030+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:07:13.080+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.091 seconds
[2024-04-30T22:07:43.993+0000] {processor.py:161} INFO - Started process (PID=12938) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:07:44.001+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:07:44.004+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:07:44.003+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:07:44.028+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:07:44.023+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:07:44.029+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:07:44.088+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.105 seconds
[2024-04-30T22:08:14.979+0000] {processor.py:161} INFO - Started process (PID=12969) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:08:14.988+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:08:14.990+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:08:14.989+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:08:15.014+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:08:15.010+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:08:15.016+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:08:15.067+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T22:08:46.014+0000] {processor.py:161} INFO - Started process (PID=13000) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:08:46.018+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:08:46.021+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:08:46.020+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:08:46.045+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:08:46.040+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:08:46.046+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:08:46.098+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T22:09:16.998+0000] {processor.py:161} INFO - Started process (PID=13031) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:09:17.000+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:09:17.003+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:09:17.002+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:09:17.027+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:09:17.022+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:09:17.028+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:09:17.080+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T22:09:48.096+0000] {processor.py:161} INFO - Started process (PID=13062) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:09:48.101+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:09:48.106+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:09:48.104+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:09:48.143+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:09:48.139+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:09:48.144+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:09:48.257+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.179 seconds
[2024-04-30T22:10:19.099+0000] {processor.py:161} INFO - Started process (PID=13093) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:10:19.102+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:10:19.104+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:10:19.103+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:10:19.129+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:10:19.124+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:10:19.130+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:10:19.181+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.091 seconds
[2024-04-30T22:10:50.148+0000] {processor.py:161} INFO - Started process (PID=13130) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:10:50.152+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:10:50.154+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:10:50.153+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:10:50.178+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:10:50.174+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:10:50.180+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:10:50.230+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T22:11:20.877+0000] {processor.py:161} INFO - Started process (PID=13161) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:11:20.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:11:20.885+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:11:20.883+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:11:20.909+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:11:20.905+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:11:20.911+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:11:20.963+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T22:11:51.926+0000] {processor.py:161} INFO - Started process (PID=13192) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:11:51.930+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:11:51.933+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:11:51.932+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:11:51.957+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:11:51.952+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:11:51.958+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:11:52.009+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T22:12:22.918+0000] {processor.py:161} INFO - Started process (PID=13223) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:12:22.922+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:12:22.925+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:12:22.924+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:12:22.949+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:12:22.944+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:12:22.950+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:12:23.001+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T22:12:53.923+0000] {processor.py:161} INFO - Started process (PID=13254) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:12:53.925+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:12:53.928+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:12:53.927+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:12:53.953+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:12:53.948+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:12:53.955+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:12:54.007+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T22:13:25.021+0000] {processor.py:161} INFO - Started process (PID=13285) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:13:25.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:13:25.028+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:13:25.027+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:13:25.052+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:13:25.048+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:13:25.054+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:13:25.106+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T22:13:56.034+0000] {processor.py:161} INFO - Started process (PID=13316) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:13:56.037+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:13:56.040+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:13:56.039+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:13:56.064+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:13:56.059+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:13:56.065+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:13:56.118+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T22:14:27.031+0000] {processor.py:161} INFO - Started process (PID=13347) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:14:27.035+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:14:27.038+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:14:27.037+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:14:27.062+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:14:27.057+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:14:27.063+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:14:27.115+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T22:14:58.007+0000] {processor.py:161} INFO - Started process (PID=13378) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:14:58.012+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:14:58.017+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:14:58.016+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:14:58.050+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:14:58.041+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:14:58.056+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:14:58.112+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T22:15:29.003+0000] {processor.py:161} INFO - Started process (PID=13413) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:15:29.005+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:15:29.008+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:15:29.007+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:15:29.032+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:15:29.027+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:15:29.033+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:15:29.085+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T22:16:00.057+0000] {processor.py:161} INFO - Started process (PID=13444) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:16:00.061+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:16:00.064+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:16:00.063+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:16:00.092+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:16:00.086+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:16:00.093+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:16:00.155+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.109 seconds
[2024-04-30T22:16:31.029+0000] {processor.py:161} INFO - Started process (PID=13475) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:16:31.032+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:16:31.035+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:16:31.034+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:16:31.059+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:16:31.054+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:16:31.060+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:16:31.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T22:17:02.073+0000] {processor.py:161} INFO - Started process (PID=13506) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:17:02.076+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:17:02.079+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:17:02.078+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:17:02.103+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:17:02.099+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:17:02.105+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:17:02.156+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T22:17:33.100+0000] {processor.py:161} INFO - Started process (PID=13537) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:17:33.102+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:17:33.105+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:17:33.104+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:17:33.129+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:17:33.125+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:17:33.131+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:17:33.182+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T22:18:04.085+0000] {processor.py:161} INFO - Started process (PID=13568) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:18:04.088+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:18:04.091+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:18:04.090+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:18:04.115+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:18:04.111+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:18:04.116+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:18:04.168+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T22:18:35.123+0000] {processor.py:161} INFO - Started process (PID=13599) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:18:35.126+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:18:35.129+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:18:35.128+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:18:35.153+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:18:35.148+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:18:35.154+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:18:35.205+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T22:19:06.156+0000] {processor.py:161} INFO - Started process (PID=13630) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:19:06.159+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:19:06.162+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:19:06.161+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:19:06.187+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:19:06.182+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:19:06.188+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:19:06.240+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T22:19:37.201+0000] {processor.py:161} INFO - Started process (PID=13661) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:19:37.205+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:19:37.208+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:19:37.207+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:19:37.232+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:19:37.227+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:19:37.233+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:19:37.285+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T22:20:08.202+0000] {processor.py:161} INFO - Started process (PID=13692) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:20:08.206+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:20:08.210+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:20:08.209+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:20:08.234+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:20:08.229+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:20:08.235+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:20:08.288+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T22:20:39.323+0000] {processor.py:161} INFO - Started process (PID=13723) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:20:39.327+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:20:39.330+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:20:39.329+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:20:39.355+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:20:39.350+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:20:39.356+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:20:39.408+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T22:21:09.506+0000] {processor.py:161} INFO - Started process (PID=13754) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:21:09.509+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:21:09.512+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:21:09.511+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:21:09.539+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:21:09.534+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:21:09.540+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:21:09.594+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.100 seconds
[2024-04-30T22:21:40.489+0000] {processor.py:161} INFO - Started process (PID=13785) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:21:40.492+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:21:40.495+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:21:40.494+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:21:40.520+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:21:40.515+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:21:40.521+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:21:40.573+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T22:22:11.546+0000] {processor.py:161} INFO - Started process (PID=13816) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:22:11.550+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:22:11.553+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:22:11.552+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:22:11.577+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:22:11.572+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:22:11.578+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:22:11.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T22:22:42.292+0000] {processor.py:161} INFO - Started process (PID=13848) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:22:42.295+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:22:42.298+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:22:42.297+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:22:42.323+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:22:42.318+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:22:42.324+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:22:42.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T22:23:13.327+0000] {processor.py:161} INFO - Started process (PID=13879) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:23:13.330+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:23:13.333+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:23:13.332+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:23:13.358+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:23:13.353+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:23:13.359+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:23:13.411+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T22:23:44.420+0000] {processor.py:161} INFO - Started process (PID=13910) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:23:44.423+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:23:44.426+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:23:44.425+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:23:44.456+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:23:44.451+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:23:44.457+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:23:44.522+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.112 seconds
[2024-04-30T22:24:15.387+0000] {processor.py:161} INFO - Started process (PID=13941) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:24:15.391+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:24:15.394+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:24:15.393+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:24:15.418+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:24:15.414+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:24:15.420+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:24:15.474+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.099 seconds
[2024-04-30T22:24:46.171+0000] {processor.py:161} INFO - Started process (PID=13972) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:24:46.175+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:24:46.178+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:24:46.177+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:24:46.202+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:24:46.198+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:24:46.204+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:24:46.257+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T22:25:17.222+0000] {processor.py:161} INFO - Started process (PID=14003) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:25:17.225+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:25:17.228+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:25:17.227+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:25:17.252+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:25:17.248+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:25:17.254+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:25:17.306+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T22:25:48.288+0000] {processor.py:161} INFO - Started process (PID=14034) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:25:48.292+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:25:48.295+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:25:48.294+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:25:48.320+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:25:48.315+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:25:48.321+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:25:48.373+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T22:26:19.288+0000] {processor.py:161} INFO - Started process (PID=14065) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:26:19.291+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:26:19.294+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:26:19.293+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:26:19.318+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:26:19.313+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:26:19.319+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:26:19.370+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T22:26:50.289+0000] {processor.py:161} INFO - Started process (PID=14096) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:26:50.297+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:26:50.300+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:26:50.299+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:26:50.325+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:26:50.320+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:26:50.326+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:26:50.381+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.103 seconds
[2024-04-30T22:27:21.340+0000] {processor.py:161} INFO - Started process (PID=14127) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:27:21.343+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:27:21.346+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:27:21.345+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:27:21.370+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:27:21.366+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:27:21.371+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:27:21.422+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T22:27:52.333+0000] {processor.py:161} INFO - Started process (PID=14158) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:27:52.336+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:27:52.339+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:27:52.338+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:27:52.363+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:27:52.359+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:27:52.365+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:27:52.417+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T22:28:23.326+0000] {processor.py:161} INFO - Started process (PID=14189) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:28:23.330+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:28:23.334+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:28:23.333+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:28:23.358+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:28:23.353+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:28:23.360+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:28:23.412+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T22:28:54.379+0000] {processor.py:161} INFO - Started process (PID=14221) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:28:54.383+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:28:54.385+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:28:54.384+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:28:54.410+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:28:54.405+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:28:54.411+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:28:54.463+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T22:29:25.408+0000] {processor.py:161} INFO - Started process (PID=14252) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:29:25.411+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:29:25.414+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:29:25.413+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:29:25.443+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:29:25.437+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:29:25.447+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:29:25.528+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.130 seconds
[2024-04-30T22:29:56.463+0000] {processor.py:161} INFO - Started process (PID=14289) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:29:56.466+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:29:56.469+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:29:56.468+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:29:56.493+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:29:56.489+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:29:56.494+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:29:56.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T22:30:27.172+0000] {processor.py:161} INFO - Started process (PID=14320) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:30:27.176+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:30:27.180+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:30:27.179+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:30:27.206+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:30:27.201+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:30:27.207+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:30:27.259+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T22:30:58.334+0000] {processor.py:161} INFO - Started process (PID=14351) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:30:58.337+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:30:58.340+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:30:58.339+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:30:58.364+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:30:58.360+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:30:58.366+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:30:58.421+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T22:31:29.346+0000] {processor.py:161} INFO - Started process (PID=14382) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:31:29.349+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:31:29.352+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:31:29.351+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:31:29.377+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:31:29.372+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:31:29.379+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:31:29.430+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T22:32:00.043+0000] {processor.py:161} INFO - Started process (PID=14413) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:32:00.047+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:32:00.050+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:32:00.049+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:32:00.074+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:32:00.070+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:32:00.076+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:32:00.128+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T22:32:31.092+0000] {processor.py:161} INFO - Started process (PID=14444) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:32:31.095+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:32:31.097+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:32:31.096+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:32:31.122+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:32:31.117+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:32:31.123+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:32:31.175+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T22:33:02.084+0000] {processor.py:161} INFO - Started process (PID=14475) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:33:02.088+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:33:02.091+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:33:02.090+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:33:02.115+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:33:02.110+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:33:02.116+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:33:02.168+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T22:33:33.090+0000] {processor.py:161} INFO - Started process (PID=14507) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:33:33.093+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:33:33.096+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:33:33.095+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:33:33.120+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:33:33.116+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:33:33.122+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:33:33.172+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T22:34:04.095+0000] {processor.py:161} INFO - Started process (PID=14538) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:34:04.099+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:34:04.102+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:34:04.101+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:34:04.129+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:34:04.124+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:34:04.131+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:34:04.184+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.100 seconds
[2024-04-30T22:34:35.123+0000] {processor.py:161} INFO - Started process (PID=14569) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:34:35.127+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:34:35.130+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:34:35.129+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:34:35.165+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:34:35.160+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:34:35.166+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:34:35.241+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.129 seconds
[2024-04-30T22:35:06.149+0000] {processor.py:161} INFO - Started process (PID=14601) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:35:06.152+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:35:06.155+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:35:06.154+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:35:06.180+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:35:06.175+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:35:06.181+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:35:06.235+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T22:35:37.216+0000] {processor.py:161} INFO - Started process (PID=14632) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:35:37.220+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:35:37.223+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:35:37.222+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:35:37.248+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:35:37.243+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:35:37.249+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:35:37.302+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.101 seconds
[2024-04-30T22:36:08.242+0000] {processor.py:161} INFO - Started process (PID=14663) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:36:08.245+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:36:08.248+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:36:08.247+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:36:08.272+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:36:08.267+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:36:08.273+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:36:08.324+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T22:36:39.326+0000] {processor.py:161} INFO - Started process (PID=14694) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:36:39.330+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:36:39.343+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:36:39.341+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:36:39.375+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:36:39.369+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:36:39.376+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:36:39.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.130 seconds
[2024-04-30T22:37:10.287+0000] {processor.py:161} INFO - Started process (PID=14725) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:37:10.291+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:37:10.294+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:37:10.293+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:37:10.318+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:37:10.314+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:37:10.320+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:37:10.375+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T22:37:41.049+0000] {processor.py:161} INFO - Started process (PID=14756) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:37:41.053+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:37:41.056+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:37:41.055+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:37:41.081+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:37:41.077+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:37:41.083+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:37:41.138+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.100 seconds
[2024-04-30T22:38:12.036+0000] {processor.py:161} INFO - Started process (PID=14787) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:38:12.039+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:38:12.042+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:38:12.041+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:38:12.068+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:38:12.063+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:38:12.069+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:38:12.120+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T22:38:42.276+0000] {processor.py:161} INFO - Started process (PID=14818) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:38:42.281+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:38:42.285+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:38:42.283+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:38:42.323+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:38:42.315+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:38:42.325+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:38:42.379+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.119 seconds
[2024-04-30T22:39:13.342+0000] {processor.py:161} INFO - Started process (PID=14849) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:39:13.345+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:39:13.348+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:39:13.347+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:39:13.372+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:39:13.367+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:39:13.373+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:39:13.424+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T22:39:43.504+0000] {processor.py:161} INFO - Started process (PID=14880) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:39:43.508+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:39:43.511+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:39:43.510+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:39:43.536+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:39:43.531+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:39:43.537+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:39:43.591+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T22:40:14.252+0000] {processor.py:161} INFO - Started process (PID=14901) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:40:14.256+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:40:14.260+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:40:14.259+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:40:14.289+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:40:14.284+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:40:14.291+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:40:14.353+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.123 seconds
[2024-04-30T22:40:45.386+0000] {processor.py:161} INFO - Started process (PID=14932) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:40:45.391+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:40:45.394+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:40:45.393+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:40:45.422+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:40:45.417+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:40:45.424+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:40:45.486+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T22:41:16.407+0000] {processor.py:161} INFO - Started process (PID=14963) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:41:16.411+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:41:16.415+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:41:16.414+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:41:16.444+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:41:16.439+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:41:16.446+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:41:16.510+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T22:41:47.420+0000] {processor.py:161} INFO - Started process (PID=14994) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:41:47.424+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:41:47.428+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:41:47.427+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:41:47.457+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:41:47.452+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:41:47.459+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:41:47.520+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T22:42:18.504+0000] {processor.py:161} INFO - Started process (PID=15026) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:42:18.508+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:42:18.513+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:42:18.511+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:42:18.546+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:42:18.539+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:42:18.547+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:42:18.609+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.121 seconds
[2024-04-30T22:42:49.524+0000] {processor.py:161} INFO - Started process (PID=15057) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:42:49.530+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:42:49.534+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:42:49.532+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:42:49.562+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:42:49.557+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:42:49.564+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:42:49.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.120 seconds
[2024-04-30T22:43:20.584+0000] {processor.py:161} INFO - Started process (PID=15088) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:43:20.589+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:43:20.593+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:43:20.591+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:43:20.622+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:43:20.616+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:43:20.623+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:43:20.691+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.122 seconds
[2024-04-30T22:43:51.625+0000] {processor.py:161} INFO - Started process (PID=15119) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:43:51.630+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:43:51.634+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:43:51.633+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:43:51.662+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:43:51.657+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:43:51.664+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:43:51.726+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T22:44:22.630+0000] {processor.py:161} INFO - Started process (PID=15150) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:44:22.635+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:44:22.639+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:44:22.638+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:44:22.669+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:44:22.663+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:44:22.670+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:44:22.733+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T22:44:53.700+0000] {processor.py:161} INFO - Started process (PID=15181) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:44:53.705+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:44:53.709+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:44:53.707+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:44:53.736+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:44:53.731+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:44:53.737+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:44:53.800+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T22:45:24.780+0000] {processor.py:161} INFO - Started process (PID=15212) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:45:24.785+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:45:24.788+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:45:24.787+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:45:24.816+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:45:24.810+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:45:24.817+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:45:24.880+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T22:45:55.792+0000] {processor.py:161} INFO - Started process (PID=15243) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:45:55.798+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:45:55.805+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:45:55.802+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:45:55.859+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:45:55.849+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:45:55.863+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:45:55.951+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.175 seconds
[2024-04-30T22:46:26.884+0000] {processor.py:161} INFO - Started process (PID=15274) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:46:26.891+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:46:26.896+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:46:26.894+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:46:26.927+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:46:26.922+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:46:26.929+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:46:26.992+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.129 seconds
[2024-04-30T22:46:58.027+0000] {processor.py:161} INFO - Started process (PID=15305) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:46:58.030+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:46:58.034+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:46:58.033+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:46:58.061+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:46:58.056+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:46:58.063+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:46:58.125+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.112 seconds
[2024-04-30T22:47:29.068+0000] {processor.py:161} INFO - Started process (PID=15336) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:47:29.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:47:29.077+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:47:29.075+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:47:29.104+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:47:29.099+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:47:29.106+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:47:29.168+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T22:48:00.145+0000] {processor.py:161} INFO - Started process (PID=15367) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:48:00.149+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:48:00.154+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:48:00.153+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:48:00.181+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:48:00.176+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:48:00.182+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:48:00.244+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T22:48:31.207+0000] {processor.py:161} INFO - Started process (PID=15398) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:48:31.212+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:48:31.216+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:48:31.214+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:48:31.243+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:48:31.238+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:48:31.245+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:48:31.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T22:49:02.207+0000] {processor.py:161} INFO - Started process (PID=15436) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:49:02.211+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:49:02.215+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:49:02.214+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:49:02.243+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:49:02.238+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:49:02.245+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:49:02.305+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T22:49:32.937+0000] {processor.py:161} INFO - Started process (PID=15467) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:49:32.941+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:49:32.944+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:49:32.943+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:49:32.972+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:49:32.966+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:49:32.973+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:49:33.036+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.111 seconds
[2024-04-30T22:50:03.998+0000] {processor.py:161} INFO - Started process (PID=15498) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:50:04.001+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:50:04.005+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:50:04.004+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:50:04.033+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:50:04.028+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:50:04.034+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:50:04.098+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T22:50:34.996+0000] {processor.py:161} INFO - Started process (PID=15529) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:50:35.001+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:50:35.007+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:50:35.004+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:50:35.046+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:50:35.038+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:50:35.049+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:50:35.126+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.144 seconds
[2024-04-30T22:51:06.012+0000] {processor.py:161} INFO - Started process (PID=15560) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:51:06.017+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:51:06.021+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:51:06.019+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:51:06.048+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:51:06.042+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:51:06.049+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:51:06.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T22:51:37.005+0000] {processor.py:161} INFO - Started process (PID=15591) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:51:37.008+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:51:37.011+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:51:37.010+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:51:37.039+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:51:37.034+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:51:37.041+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:51:37.104+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T22:52:08.028+0000] {processor.py:161} INFO - Started process (PID=15622) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:52:08.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:52:08.041+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:52:08.040+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:52:08.068+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:52:08.063+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:52:08.070+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:52:08.132+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.118 seconds
[2024-04-30T22:52:39.012+0000] {processor.py:161} INFO - Started process (PID=15653) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:52:39.016+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:52:39.020+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:52:39.019+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:52:39.049+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:52:39.043+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:52:39.050+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:52:39.112+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T22:53:10.029+0000] {processor.py:161} INFO - Started process (PID=15684) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:53:10.033+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:53:10.037+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:53:10.035+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:53:10.065+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:53:10.060+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:53:10.066+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:53:10.129+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T22:53:41.017+0000] {processor.py:161} INFO - Started process (PID=15715) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:53:41.022+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:53:41.025+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:53:41.024+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:53:41.053+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:53:41.048+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:53:41.054+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:53:41.118+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T22:54:12.072+0000] {processor.py:161} INFO - Started process (PID=15746) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:54:12.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:54:12.080+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:54:12.079+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:54:12.108+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:54:12.102+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:54:12.109+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:54:12.173+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T22:54:43.212+0000] {processor.py:161} INFO - Started process (PID=15777) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:54:43.217+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:54:43.220+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:54:43.219+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:54:43.249+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:54:43.244+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:54:43.250+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:54:43.312+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T22:55:14.232+0000] {processor.py:161} INFO - Started process (PID=15808) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:55:14.238+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:55:14.241+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:55:14.240+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:55:14.268+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:55:14.263+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:55:14.270+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:55:14.333+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T22:55:45.312+0000] {processor.py:161} INFO - Started process (PID=15839) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:55:45.315+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:55:45.319+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:55:45.317+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:55:45.346+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:55:45.341+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:55:45.347+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:55:45.411+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T22:56:16.322+0000] {processor.py:161} INFO - Started process (PID=15870) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:56:16.326+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:56:16.330+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:56:16.328+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:56:16.357+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:56:16.352+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:56:16.359+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:56:16.424+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T22:56:47.424+0000] {processor.py:161} INFO - Started process (PID=15901) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:56:47.427+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:56:47.433+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:56:47.430+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:56:47.463+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:56:47.457+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:56:47.465+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:56:47.537+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.130 seconds
[2024-04-30T22:57:18.539+0000] {processor.py:161} INFO - Started process (PID=15932) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:57:18.543+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:57:18.547+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:57:18.546+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:57:18.574+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:57:18.569+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:57:18.576+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:57:18.638+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T22:57:49.578+0000] {processor.py:161} INFO - Started process (PID=15963) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:57:49.582+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:57:49.586+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:57:49.585+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:57:49.615+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:57:49.610+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:57:49.616+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:57:49.677+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T22:58:20.629+0000] {processor.py:161} INFO - Started process (PID=15995) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:58:20.634+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:58:20.637+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:58:20.636+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:58:20.664+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:58:20.659+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:58:20.666+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:58:20.727+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T22:58:51.388+0000] {processor.py:161} INFO - Started process (PID=16026) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:58:51.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:58:51.395+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:58:51.394+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:58:51.422+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:58:51.417+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:58:51.423+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:58:51.485+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.111 seconds
[2024-04-30T22:59:22.393+0000] {processor.py:161} INFO - Started process (PID=16057) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:59:22.397+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:59:22.401+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:59:22.400+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:59:22.429+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:59:22.424+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:59:22.430+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:59:22.515+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.138 seconds
[2024-04-30T22:59:53.458+0000] {processor.py:161} INFO - Started process (PID=16088) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T22:59:53.462+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T22:59:53.466+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:59:53.465+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:59:53.494+0000] {logging_mixin.py:188} INFO - [2024-04-30T22:59:53.488+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T22:59:53.495+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T22:59:53.557+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T23:00:24.443+0000] {processor.py:161} INFO - Started process (PID=16119) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:00:24.446+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:00:24.450+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:00:24.449+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:00:24.479+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:00:24.474+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:00:24.480+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:00:24.542+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T23:00:55.422+0000] {processor.py:161} INFO - Started process (PID=16150) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:00:55.426+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:00:55.430+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:00:55.429+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:00:55.460+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:00:55.454+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:00:55.461+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:00:55.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T23:01:26.457+0000] {processor.py:161} INFO - Started process (PID=16180) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:01:26.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:01:26.465+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:01:26.463+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:01:26.493+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:01:26.488+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:01:26.494+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:01:26.555+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T23:01:57.494+0000] {processor.py:161} INFO - Started process (PID=16211) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:01:57.498+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:01:57.503+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:01:57.501+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:01:57.535+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:01:57.529+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:01:57.536+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:01:57.603+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.124 seconds
[2024-04-30T23:02:28.516+0000] {processor.py:161} INFO - Started process (PID=16242) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:02:28.520+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:02:28.525+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:02:28.523+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:02:28.553+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:02:28.548+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:02:28.554+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:02:28.616+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T23:02:59.522+0000] {processor.py:161} INFO - Started process (PID=16273) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:02:59.526+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:02:59.530+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:02:59.528+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:02:59.558+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:02:59.553+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:02:59.560+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:02:59.623+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T23:03:30.634+0000] {processor.py:161} INFO - Started process (PID=16304) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:03:30.639+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:03:30.645+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:03:30.643+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:03:30.672+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:03:30.667+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:03:30.673+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:03:30.738+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.118 seconds
[2024-04-30T23:04:01.634+0000] {processor.py:161} INFO - Started process (PID=16335) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:04:01.639+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:04:01.643+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:04:01.641+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:04:01.670+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:04:01.665+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:04:01.672+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:04:01.734+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T23:04:32.643+0000] {processor.py:161} INFO - Started process (PID=16366) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:04:32.647+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:04:32.651+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:04:32.650+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:04:32.679+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:04:32.673+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:04:32.680+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:04:32.741+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T23:05:03.841+0000] {processor.py:161} INFO - Started process (PID=16397) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:05:03.846+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:05:03.851+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:05:03.849+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:05:03.886+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:05:03.880+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:05:03.888+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:05:03.952+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.129 seconds
[2024-04-30T23:05:34.887+0000] {processor.py:161} INFO - Started process (PID=16428) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:05:34.892+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:05:34.895+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:05:34.894+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:05:34.922+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:05:34.917+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:05:34.924+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:05:34.984+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T23:06:05.929+0000] {processor.py:161} INFO - Started process (PID=16459) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:06:05.933+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:06:05.937+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:06:05.935+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:06:05.964+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:06:05.959+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:06:05.965+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:06:06.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.112 seconds
[2024-04-30T23:06:37.022+0000] {processor.py:161} INFO - Started process (PID=16490) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:06:37.026+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:06:37.030+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:06:37.029+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:06:37.057+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:06:37.052+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:06:37.059+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:06:37.121+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T23:07:08.019+0000] {processor.py:161} INFO - Started process (PID=16528) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:07:08.023+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:07:08.027+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:07:08.026+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:07:08.055+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:07:08.050+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:07:08.057+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:07:08.148+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.144 seconds
[2024-04-30T23:07:38.794+0000] {processor.py:161} INFO - Started process (PID=16560) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:07:38.798+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:07:38.801+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:07:38.800+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:07:38.828+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:07:38.823+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:07:38.830+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:07:38.892+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.111 seconds
[2024-04-30T23:08:09.806+0000] {processor.py:161} INFO - Started process (PID=16591) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:08:09.810+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:08:09.814+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:08:09.813+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:08:09.842+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:08:09.836+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:08:09.843+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:08:09.908+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T23:08:40.860+0000] {processor.py:161} INFO - Started process (PID=16622) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:08:40.867+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:08:40.871+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:08:40.870+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:08:40.901+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:08:40.893+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:08:40.904+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:08:40.977+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.143 seconds
[2024-04-30T23:09:11.602+0000] {processor.py:161} INFO - Started process (PID=16653) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:09:11.605+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:09:11.608+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:09:11.607+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:09:11.636+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:09:11.631+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:09:11.637+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:09:11.703+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T23:09:42.686+0000] {processor.py:161} INFO - Started process (PID=16684) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:09:42.690+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:09:42.694+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:09:42.693+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:09:42.721+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:09:42.716+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:09:42.723+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:09:42.784+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T23:10:13.745+0000] {processor.py:161} INFO - Started process (PID=16715) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:10:13.750+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:10:13.754+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:10:13.752+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:10:13.781+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:10:13.776+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:10:13.783+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:10:13.844+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T23:10:44.750+0000] {processor.py:161} INFO - Started process (PID=16746) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:10:44.753+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:10:44.757+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:10:44.756+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:10:44.784+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:10:44.779+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:10:44.786+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:10:44.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T23:11:15.810+0000] {processor.py:161} INFO - Started process (PID=16777) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:11:15.815+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:11:15.819+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:11:15.818+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:11:15.849+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:11:15.843+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:11:15.851+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:11:15.922+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.128 seconds
[2024-04-30T23:11:46.834+0000] {processor.py:161} INFO - Started process (PID=16808) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:11:46.838+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:11:46.842+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:11:46.841+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:11:46.878+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:11:46.872+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:11:46.880+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:11:46.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.147 seconds
[2024-04-30T23:12:17.875+0000] {processor.py:161} INFO - Started process (PID=16839) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:12:17.880+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:12:17.884+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:12:17.882+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:12:17.911+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:12:17.906+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:12:17.912+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:12:17.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T23:12:48.689+0000] {processor.py:161} INFO - Started process (PID=16870) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:12:48.692+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:12:48.695+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:12:48.694+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:12:48.722+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:12:48.717+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:12:48.724+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:12:48.786+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.111 seconds
[2024-04-30T23:13:19.456+0000] {processor.py:161} INFO - Started process (PID=16901) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:13:19.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:13:19.463+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:13:19.462+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:13:19.490+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:13:19.485+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:13:19.491+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:13:19.553+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.110 seconds
[2024-04-30T23:13:50.558+0000] {processor.py:161} INFO - Started process (PID=16932) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:13:50.562+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:13:50.566+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:13:50.565+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:13:50.597+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:13:50.591+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:13:50.599+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:13:50.672+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.132 seconds
[2024-04-30T23:14:21.610+0000] {processor.py:161} INFO - Started process (PID=16963) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:14:21.614+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:14:21.618+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:14:21.617+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:14:21.646+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:14:21.640+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:14:21.647+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:14:21.713+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T23:14:52.694+0000] {processor.py:161} INFO - Started process (PID=16993) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:14:52.698+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:14:52.702+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:14:52.700+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:14:52.730+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:14:52.725+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:14:52.732+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:14:52.797+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T23:15:23.777+0000] {processor.py:161} INFO - Started process (PID=17026) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:15:23.781+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:15:23.785+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:15:23.784+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:15:23.812+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:15:23.807+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:15:23.813+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:15:23.889+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.128 seconds
[2024-04-30T23:15:54.806+0000] {processor.py:161} INFO - Started process (PID=17057) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:15:54.810+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:15:54.814+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:15:54.812+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:15:54.841+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:15:54.836+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:15:54.842+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:15:54.906+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T23:16:25.838+0000] {processor.py:161} INFO - Started process (PID=17088) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:16:25.842+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:16:25.846+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:16:25.844+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:16:25.873+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:16:25.868+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:16:25.874+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:16:25.936+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.112 seconds
[2024-04-30T23:16:56.853+0000] {processor.py:161} INFO - Started process (PID=17119) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:16:56.857+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:16:56.861+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:16:56.860+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:16:56.888+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:16:56.883+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:16:56.890+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:16:56.954+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T23:17:27.894+0000] {processor.py:161} INFO - Started process (PID=17151) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:17:27.899+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:17:27.902+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:17:27.901+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:17:27.929+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:17:27.924+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:17:27.931+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:17:28.021+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.143 seconds
[2024-04-30T23:17:58.953+0000] {processor.py:161} INFO - Started process (PID=17182) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:17:58.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:17:58.963+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:17:58.961+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:17:58.992+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:17:58.986+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:17:58.993+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:17:59.054+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.118 seconds
[2024-04-30T23:18:29.970+0000] {processor.py:161} INFO - Started process (PID=17213) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:18:29.974+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:18:29.978+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:18:29.977+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:18:30.005+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:18:30.000+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:18:30.007+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:18:30.068+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.112 seconds
[2024-04-30T23:19:01.003+0000] {processor.py:161} INFO - Started process (PID=17244) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:19:01.007+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:19:01.011+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:19:01.009+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:19:01.038+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:19:01.033+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:19:01.039+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:19:01.103+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T23:19:32.052+0000] {processor.py:161} INFO - Started process (PID=17275) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:19:32.057+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:19:32.060+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:19:32.059+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:19:32.087+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:19:32.082+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:19:32.089+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:19:32.153+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T23:20:03.205+0000] {processor.py:161} INFO - Started process (PID=17306) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:20:03.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:20:03.213+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:20:03.211+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:20:03.240+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:20:03.234+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:20:03.241+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:20:03.306+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.118 seconds
[2024-04-30T23:20:34.243+0000] {processor.py:161} INFO - Started process (PID=17337) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:20:34.248+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:20:34.252+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:20:34.250+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:20:34.281+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:20:34.276+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:20:34.282+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:20:34.346+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T23:21:05.253+0000] {processor.py:161} INFO - Started process (PID=17368) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:21:05.258+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:21:05.262+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:21:05.260+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:21:05.289+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:21:05.284+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:21:05.290+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:21:05.356+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.118 seconds
[2024-04-30T23:21:36.281+0000] {processor.py:161} INFO - Started process (PID=17399) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:21:36.286+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:21:36.290+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:21:36.288+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:21:36.317+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:21:36.312+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:21:36.318+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:21:36.380+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T23:22:07.005+0000] {processor.py:161} INFO - Started process (PID=17430) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:22:07.009+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:22:07.012+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:22:07.011+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:22:07.041+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:22:07.035+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:22:07.042+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:22:07.102+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.110 seconds
[2024-04-30T23:22:38.040+0000] {processor.py:161} INFO - Started process (PID=17462) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:22:38.045+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:22:38.049+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:22:38.047+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:22:38.076+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:22:38.071+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:22:38.077+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:22:38.138+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.112 seconds
[2024-04-30T23:23:09.045+0000] {processor.py:161} INFO - Started process (PID=17493) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:23:09.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:23:09.053+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:23:09.051+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:23:09.082+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:23:09.076+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:23:09.083+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:23:09.147+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T23:23:40.119+0000] {processor.py:161} INFO - Started process (PID=17524) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:23:40.124+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:23:40.127+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:23:40.126+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:23:40.155+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:23:40.150+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:23:40.156+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:23:40.220+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.117 seconds
[2024-04-30T23:24:11.154+0000] {processor.py:161} INFO - Started process (PID=17555) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:24:11.159+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:24:11.163+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:24:11.161+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:24:11.189+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:24:11.184+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:24:11.191+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:24:11.252+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.112 seconds
[2024-04-30T23:24:42.274+0000] {processor.py:161} INFO - Started process (PID=17586) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:24:42.285+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:24:42.289+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:24:42.288+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:24:42.316+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:24:42.311+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:24:42.317+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:24:42.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.118 seconds
[2024-04-30T23:25:13.034+0000] {processor.py:161} INFO - Started process (PID=17617) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:25:13.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:25:13.041+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:25:13.040+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:25:13.071+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:25:13.065+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:25:13.072+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:25:13.136+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.118 seconds
[2024-04-30T23:25:44.063+0000] {processor.py:161} INFO - Started process (PID=17655) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:25:44.067+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:25:44.071+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:25:44.070+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:25:44.099+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:25:44.094+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:25:44.100+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:25:44.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T23:26:14.837+0000] {processor.py:161} INFO - Started process (PID=17687) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:26:14.840+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:26:14.843+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:26:14.842+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:26:14.871+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:26:14.865+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:26:14.872+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:26:14.934+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.111 seconds
[2024-04-30T23:26:45.846+0000] {processor.py:161} INFO - Started process (PID=17719) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:26:45.850+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:26:45.854+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:26:45.853+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:26:45.882+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:26:45.876+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:26:45.883+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:26:45.945+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T23:27:16.848+0000] {processor.py:161} INFO - Started process (PID=17750) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:27:16.853+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:27:16.857+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:27:16.855+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:27:16.887+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:27:16.881+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:27:16.889+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:27:16.957+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.124 seconds
[2024-04-30T23:27:47.925+0000] {processor.py:161} INFO - Started process (PID=17781) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:27:47.929+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:27:47.933+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:27:47.931+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:27:47.960+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:27:47.955+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:27:47.962+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:27:48.024+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T23:28:18.914+0000] {processor.py:161} INFO - Started process (PID=17812) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:28:18.917+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:28:18.921+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:28:18.919+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:28:18.948+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:28:18.943+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:28:18.950+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:28:19.014+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T23:28:49.922+0000] {processor.py:161} INFO - Started process (PID=17843) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:28:49.931+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:28:49.935+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:28:49.933+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:28:49.962+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:28:49.957+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:28:49.963+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:28:50.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.119 seconds
[2024-04-30T23:29:21.013+0000] {processor.py:161} INFO - Started process (PID=17874) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:29:21.017+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:29:21.021+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:29:21.019+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:29:21.056+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:29:21.050+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:29:21.058+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:29:21.147+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.149 seconds
[2024-04-30T23:29:52.144+0000] {processor.py:161} INFO - Started process (PID=17905) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:29:52.148+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:29:52.152+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:29:52.150+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:29:52.180+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:29:52.174+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:29:52.181+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:29:52.243+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T23:30:23.208+0000] {processor.py:161} INFO - Started process (PID=17936) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:30:23.213+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:30:23.217+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:30:23.216+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:30:23.245+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:30:23.240+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:30:23.246+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:30:23.310+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.116 seconds
[2024-04-30T23:30:54.225+0000] {processor.py:161} INFO - Started process (PID=17967) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:30:54.229+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:30:54.235+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:30:54.233+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:30:54.262+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:30:54.257+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:30:54.263+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:30:54.325+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T23:31:25.342+0000] {processor.py:161} INFO - Started process (PID=17998) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:31:25.346+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:31:25.350+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:31:25.349+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:31:25.378+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:31:25.373+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:31:25.379+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:31:25.449+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.121 seconds
[2024-04-30T23:31:56.046+0000] {processor.py:161} INFO - Started process (PID=18029) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:31:56.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:31:56.053+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:31:56.051+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:31:56.080+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:31:56.075+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:31:56.081+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:31:56.143+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.110 seconds
[2024-04-30T23:32:27.163+0000] {processor.py:161} INFO - Started process (PID=18060) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:32:27.167+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:32:27.171+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:32:27.169+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:32:27.199+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:32:27.193+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:32:27.201+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:32:27.271+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.123 seconds
[2024-04-30T23:32:58.203+0000] {processor.py:161} INFO - Started process (PID=18091) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:32:58.208+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:32:58.211+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:32:58.210+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:32:58.239+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:32:58.233+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:32:58.240+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:32:58.308+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.120 seconds
[2024-04-30T23:33:29.273+0000] {processor.py:161} INFO - Started process (PID=18122) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:33:29.277+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:33:29.281+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:33:29.280+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:33:29.308+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:33:29.303+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:33:29.310+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:33:29.372+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T23:33:59.859+0000] {processor.py:161} INFO - Started process (PID=18153) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:33:59.863+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:33:59.865+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:33:59.864+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:33:59.890+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:33:59.885+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:33:59.891+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:33:59.943+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T23:34:30.893+0000] {processor.py:161} INFO - Started process (PID=18184) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:34:30.897+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:34:30.899+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:34:30.898+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:34:30.923+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:34:30.919+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:34:30.924+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:34:30.976+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T23:35:01.974+0000] {processor.py:161} INFO - Started process (PID=18215) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:35:01.979+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:35:01.985+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:35:01.983+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:35:02.038+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:35:02.031+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:35:02.040+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:35:02.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.141 seconds
[2024-04-30T23:35:32.615+0000] {processor.py:161} INFO - Started process (PID=18246) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:35:32.618+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:35:32.621+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:35:32.620+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:35:32.645+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:35:32.640+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:35:32.646+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:35:32.699+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T23:36:03.321+0000] {processor.py:161} INFO - Started process (PID=18277) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:36:03.324+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:36:03.327+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:36:03.326+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:36:03.351+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:36:03.347+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:36:03.353+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:36:03.411+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.101 seconds
[2024-04-30T23:36:34.361+0000] {processor.py:161} INFO - Started process (PID=18308) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:36:34.365+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:36:34.367+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:36:34.366+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:36:34.391+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:36:34.387+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:36:34.393+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:36:34.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T23:37:04.584+0000] {processor.py:161} INFO - Started process (PID=18339) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:37:04.587+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:37:04.590+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:37:04.589+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:37:04.616+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:37:04.611+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:37:04.617+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:37:04.669+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T23:37:35.242+0000] {processor.py:161} INFO - Started process (PID=18360) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:37:35.246+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:37:35.250+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:37:35.248+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:37:35.278+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:37:35.273+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:37:35.279+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:37:35.343+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.115 seconds
[2024-04-30T23:38:06.214+0000] {processor.py:161} INFO - Started process (PID=18391) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:38:06.224+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:38:06.228+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:38:06.226+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:38:06.255+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:38:06.250+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:38:06.256+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:38:06.320+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.123 seconds
[2024-04-30T23:38:37.328+0000] {processor.py:161} INFO - Started process (PID=18422) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:38:37.333+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:38:37.337+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:38:37.335+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:38:37.365+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:38:37.359+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:38:37.366+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:38:37.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.114 seconds
[2024-04-30T23:39:07.542+0000] {processor.py:161} INFO - Started process (PID=18453) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:39:07.545+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:39:07.548+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:39:07.547+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:39:07.572+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:39:07.567+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:39:07.573+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:39:07.625+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T23:39:38.645+0000] {processor.py:161} INFO - Started process (PID=18484) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:39:38.649+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:39:38.651+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:39:38.650+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:39:38.675+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:39:38.671+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:39:38.676+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:39:38.729+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T23:40:08.841+0000] {processor.py:161} INFO - Started process (PID=18516) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:40:08.845+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:40:08.848+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:40:08.847+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:40:08.872+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:40:08.867+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:40:08.873+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:40:08.925+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T23:40:39.963+0000] {processor.py:161} INFO - Started process (PID=18548) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:40:39.966+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:40:39.969+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:40:39.968+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:40:39.993+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:40:39.988+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:40:39.994+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:40:40.044+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.090 seconds
[2024-04-30T23:41:10.954+0000] {processor.py:161} INFO - Started process (PID=18579) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:41:10.957+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:41:10.960+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:41:10.959+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:41:10.984+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:41:10.979+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:41:10.985+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:41:11.036+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T23:41:42.067+0000] {processor.py:161} INFO - Started process (PID=18610) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:41:42.070+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:41:42.073+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:41:42.072+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:41:42.097+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:41:42.092+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:41:42.098+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:41:42.149+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T23:42:12.641+0000] {processor.py:161} INFO - Started process (PID=18641) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:42:12.644+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:42:12.646+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:42:12.645+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:42:12.672+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:42:12.667+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:42:12.673+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:42:12.740+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.110 seconds
[2024-04-30T23:42:43.107+0000] {processor.py:161} INFO - Started process (PID=18672) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:42:43.111+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:42:43.114+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:42:43.113+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:42:43.138+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:42:43.133+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:42:43.139+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:42:43.195+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T23:43:13.511+0000] {processor.py:161} INFO - Started process (PID=18703) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:43:13.515+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:43:13.517+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:43:13.516+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:43:13.542+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:43:13.537+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:43:13.543+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:43:13.596+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.096 seconds
[2024-04-30T23:43:43.712+0000] {processor.py:161} INFO - Started process (PID=18734) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:43:43.716+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:43:43.721+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:43:43.719+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:43:43.749+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:43:43.744+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:43:43.750+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:43:43.807+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.112 seconds
[2024-04-30T23:44:14.764+0000] {processor.py:161} INFO - Started process (PID=18765) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:44:14.767+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:44:14.770+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:44:14.769+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:44:14.794+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:44:14.789+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:44:14.795+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:44:14.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T23:44:45.835+0000] {processor.py:161} INFO - Started process (PID=18796) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:44:45.837+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:44:45.840+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:44:45.839+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:44:45.864+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:44:45.859+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:44:45.865+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:44:45.918+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T23:45:16.885+0000] {processor.py:161} INFO - Started process (PID=18828) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:45:16.888+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:45:16.891+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:45:16.890+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:45:16.915+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:45:16.910+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:45:16.916+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:45:16.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T23:45:47.640+0000] {processor.py:161} INFO - Started process (PID=18859) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:45:47.643+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:45:47.645+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:45:47.644+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:45:47.670+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:45:47.665+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:45:47.671+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:45:47.726+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T23:46:18.609+0000] {processor.py:161} INFO - Started process (PID=18890) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:46:18.612+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:46:18.615+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:46:18.614+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:46:18.640+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:46:18.635+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:46:18.641+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:46:18.691+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T23:46:49.660+0000] {processor.py:161} INFO - Started process (PID=18921) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:46:49.663+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:46:49.666+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:46:49.665+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:46:49.690+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:46:49.685+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:46:49.691+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:46:49.743+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T23:47:20.668+0000] {processor.py:161} INFO - Started process (PID=18958) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:47:20.671+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:47:20.674+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:47:20.673+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:47:20.700+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:47:20.695+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:47:20.701+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:47:20.752+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T23:47:51.657+0000] {processor.py:161} INFO - Started process (PID=18989) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:47:51.660+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:47:51.663+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:47:51.662+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:47:51.690+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:47:51.685+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:47:51.691+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:47:51.743+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.097 seconds
[2024-04-30T23:48:22.773+0000] {processor.py:161} INFO - Started process (PID=19021) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:48:22.777+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:48:22.779+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:48:22.778+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:48:22.803+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:48:22.798+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:48:22.804+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:48:22.858+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T23:48:53.824+0000] {processor.py:161} INFO - Started process (PID=19052) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:48:53.827+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:48:53.829+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:48:53.828+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:48:53.853+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:48:53.849+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:48:53.855+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:48:53.905+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.091 seconds
[2024-04-30T23:49:24.965+0000] {processor.py:161} INFO - Started process (PID=19083) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:49:24.968+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:49:24.971+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:49:24.970+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:49:24.995+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:49:24.990+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:49:24.996+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:49:25.050+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T23:49:55.799+0000] {processor.py:161} INFO - Started process (PID=19114) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:49:55.802+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:49:55.805+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:49:55.804+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:49:55.829+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:49:55.824+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:49:55.830+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:49:55.916+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.127 seconds
[2024-04-30T23:50:26.784+0000] {processor.py:161} INFO - Started process (PID=19145) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:50:26.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:50:26.788+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:50:26.787+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:50:26.812+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:50:26.808+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:50:26.813+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:50:26.864+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.091 seconds
[2024-04-30T23:50:57.814+0000] {processor.py:161} INFO - Started process (PID=19176) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:50:57.817+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:50:57.820+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:50:57.819+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:50:57.844+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:50:57.840+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:50:57.845+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:50:57.896+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T23:51:28.896+0000] {processor.py:161} INFO - Started process (PID=19207) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:51:28.898+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:51:28.901+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:51:28.900+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:51:28.926+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:51:28.921+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:51:28.927+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:51:28.978+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.100 seconds
[2024-04-30T23:51:59.872+0000] {processor.py:161} INFO - Started process (PID=19238) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:51:59.875+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:51:59.878+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:51:59.877+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:51:59.902+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:51:59.897+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:51:59.903+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:51:59.955+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T23:52:30.879+0000] {processor.py:161} INFO - Started process (PID=19269) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:52:30.883+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:52:30.885+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:52:30.884+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:52:30.910+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:52:30.905+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:52:30.911+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:52:30.996+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.126 seconds
[2024-04-30T23:53:01.872+0000] {processor.py:161} INFO - Started process (PID=19300) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:53:01.875+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:53:01.878+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:53:01.877+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:53:01.902+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:53:01.897+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:53:01.903+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:53:01.953+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.091 seconds
[2024-04-30T23:53:32.559+0000] {processor.py:161} INFO - Started process (PID=19331) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:53:32.563+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:53:32.565+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:53:32.564+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:53:32.590+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:53:32.585+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:53:32.591+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:53:32.644+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T23:54:03.558+0000] {processor.py:161} INFO - Started process (PID=19362) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:54:03.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:54:03.564+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:54:03.563+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:54:03.588+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:54:03.584+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:54:03.590+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:54:03.641+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T23:54:34.551+0000] {processor.py:161} INFO - Started process (PID=19393) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:54:34.554+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:54:34.557+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:54:34.556+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:54:34.581+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:54:34.576+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:54:34.582+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:54:34.636+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.095 seconds
[2024-04-30T23:55:05.614+0000] {processor.py:161} INFO - Started process (PID=19424) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:55:05.617+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:55:05.619+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:55:05.618+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:55:05.643+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:55:05.639+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:55:05.644+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:55:05.695+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.090 seconds
[2024-04-30T23:55:36.640+0000] {processor.py:161} INFO - Started process (PID=19455) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:55:36.643+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:55:36.647+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:55:36.645+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:55:36.674+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:55:36.668+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:55:36.675+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:55:36.727+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.098 seconds
[2024-04-30T23:56:07.680+0000] {processor.py:161} INFO - Started process (PID=19486) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:56:07.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:56:07.687+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:56:07.686+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:56:07.718+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:56:07.713+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:56:07.719+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:56:07.780+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.113 seconds
[2024-04-30T23:56:38.681+0000] {processor.py:161} INFO - Started process (PID=19517) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:56:38.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:56:38.686+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:56:38.685+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:56:38.710+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:56:38.705+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:56:38.711+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:56:38.818+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.147 seconds
[2024-04-30T23:57:09.715+0000] {processor.py:161} INFO - Started process (PID=19548) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:57:09.718+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:57:09.721+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:57:09.720+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:57:09.745+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:57:09.740+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:57:09.746+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:57:09.797+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T23:57:40.756+0000] {processor.py:161} INFO - Started process (PID=19579) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:57:40.760+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:57:40.764+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:57:40.763+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:57:40.802+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:57:40.794+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:57:40.804+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:57:40.860+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.119 seconds
[2024-04-30T23:58:11.729+0000] {processor.py:161} INFO - Started process (PID=19611) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:58:11.732+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:58:11.735+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:58:11.734+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:58:11.759+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:58:11.754+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:58:11.760+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:58:11.812+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
[2024-04-30T23:58:42.482+0000] {processor.py:161} INFO - Started process (PID=19642) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:58:42.485+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:58:42.488+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:58:42.487+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:58:42.512+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:58:42.507+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:58:42.513+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:58:42.565+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.093 seconds
[2024-04-30T23:59:13.511+0000] {processor.py:161} INFO - Started process (PID=19673) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:59:13.515+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:59:13.517+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:59:13.516+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:59:13.542+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:59:13.537+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:59:13.543+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:59:13.595+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.094 seconds
[2024-04-30T23:59:44.488+0000] {processor.py:161} INFO - Started process (PID=19704) to work on /opt/airflow/dags/get_and_load.py
[2024-04-30T23:59:44.491+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/get_and_load.py for tasks to queue
[2024-04-30T23:59:44.494+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:59:44.493+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:59:44.518+0000] {logging_mixin.py:188} INFO - [2024-04-30T23:59:44.513+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/get_and_load.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/get_and_load.py", line 178, in <module>
    load_data_to_bq = _load_data_to_bq(file_path=f"/opt/airflow/dags/data_to_load.csv", dataset_id="ds525_capstaone_db", table_id="aqi_data_test")
  File "/opt/airflow/dags/get_and_load.py", line 117, in _load_data_to_bq
    load_task = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 793, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bq). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'my_bqr_conn'}
[2024-04-30T23:59:44.519+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/get_and_load.py
[2024-04-30T23:59:44.570+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/get_and_load.py took 0.092 seconds
